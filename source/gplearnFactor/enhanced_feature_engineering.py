#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆç‰¹å¾å·¥ç¨‹ - æ‰‹åŠ¨åˆ›å»ºå¤æ‚ç‰¹å¾ç»„åˆ
æ›¿ä»£GPLearnçš„åŠŸèƒ½
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mutual_info_score
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

class EnhancedFeatureExtractor:
    """å¢å¼ºç‰ˆç‰¹å¾æå–å™¨"""
    
    def __init__(self):
        self.features = None
        self.feature_names = None
        self.scaler = StandardScaler()
        
    def load_data(self, target_file, long_file, target_id='000001.SZ'):
        """åŠ è½½æ•°æ®æ–‡ä»¶"""
        print("æ­£åœ¨åŠ è½½æ•°æ®...")
        
        # åŠ è½½ç›®æ ‡æ•°æ®
        self.target_df = pd.read_csv(target_file)
        self.target_df['time'] = pd.to_datetime(self.target_df['time'])
        
        # åŠ è½½æ—¶åºæ•°æ®
        self.long_df = pd.read_csv(long_file)
        self.long_df['time'] = pd.to_datetime(self.long_df['time'])
        
        # åªä¿ç•™æŒ‡å®šIDçš„æ•°æ®
        self.target_df = self.target_df[self.target_df['id'] == target_id].copy()
        self.long_df = self.long_df[self.long_df['id'] == target_id].copy()
        
        print(f"ç›®æ ‡æ•°æ®å½¢çŠ¶: {self.target_df.shape}")
        print(f"æ—¶åºæ•°æ®å½¢çŠ¶: {self.long_df.shape}")
        print(f"å¤„ç†ç›®æ ‡ID: {target_id}")
        
        return self
    
    def prepare_features(self, window_sizes=[5, 10]):
        """å‡†å¤‡æ—¶åºç‰¹å¾"""
        print("æ­£åœ¨å‡†å¤‡æ—¶åºç‰¹å¾...")
        
        feature_dfs = []
        total_windows = len(window_sizes)
        
        for i, window in enumerate(window_sizes, 1):
            print(f"å¤„ç†çª—å£å¤§å°: {window} ({i}/{total_windows})")
            
            # è®¡ç®—æ»šåŠ¨ç»Ÿè®¡é‡
            rolling_stats = self.long_df.groupby('id')['value'].rolling(
                window=window, min_periods=1
            ).agg([
                'mean', 'std', 'min', 'max', 'median',
                'skew', 'kurt', 'var'
            ])
            
            # å•ç‹¬è®¡ç®—åˆ†ä½æ•°
            q25 = self.long_df.groupby('id')['value'].rolling(
                window=window, min_periods=1
            ).quantile(0.25)
            
            q75 = self.long_df.groupby('id')['value'].rolling(
                window=window, min_periods=1
            ).quantile(0.75)
            
            # è®¡ç®—ä»·æ ¼å˜åŒ–
            pct_change = self.long_df.groupby('id')['value'].pct_change()
            pct_change_rolling = self.long_df.groupby('id')['value'].pct_change().rolling(
                window=window, min_periods=1
            ).mean()
            
            # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
            sma = self.long_df.groupby('id')['value'].rolling(
                window=window, min_periods=1
            ).mean()
            
            ema = self.long_df.groupby('id')['value'].ewm(
                span=window, min_periods=1
            ).mean()
            
            rsi = self._calculate_rsi(window)
            
            # åˆ›å»ºç‰¹å¾DataFrame
            rolling_features = pd.DataFrame({
                'id': self.long_df['id'],
                'time': self.long_df['time'],
                'mean': rolling_stats['mean'].reset_index(0, drop=True),
                'std': rolling_stats['std'].reset_index(0, drop=True),
                'min': rolling_stats['min'].reset_index(0, drop=True),
                'max': rolling_stats['max'].reset_index(0, drop=True),
                'median': rolling_stats['median'].reset_index(0, drop=True),
                'skew': rolling_stats['skew'].reset_index(0, drop=True),
                'kurt': rolling_stats['kurt'].reset_index(0, drop=True),
                'var': rolling_stats['var'].reset_index(0, drop=True),
                'q25': q25.reset_index(0, drop=True),
                'q75': q75.reset_index(0, drop=True),
                'pct_change': pct_change.reset_index(0, drop=True),
                'pct_change_rolling': pct_change_rolling.reset_index(0, drop=True),
                'sma': sma.reset_index(0, drop=True),
                'ema': ema.reset_index(0, drop=True),
                'rsi': rsi,
                'window': window
            })
            
            feature_dfs.append(rolling_features)
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        self.features_df = pd.concat(feature_dfs, ignore_index=True)
        
        # ç¡®ä¿æ—¶é—´åˆ—æ ¼å¼ä¸€è‡´
        self.features_df['time'] = pd.to_datetime(self.features_df['time'])
        
        # ä¸ç›®æ ‡æ•°æ®åˆå¹¶
        self.merged_df = pd.merge(
            self.target_df,
            self.features_df,
            on=['id', 'time'],
            how='left'
        )
        
        print(f"åˆå¹¶åæ•°æ®å½¢çŠ¶: {self.merged_df.shape}")
        return self
    
    def _calculate_rsi(self, window):
        """è®¡ç®—RSIæŒ‡æ ‡"""
        rsi_values = []
        
        for id_val in self.long_df['id'].unique():
            id_data = self.long_df[self.long_df['id'] == id_val]['value'].values
            
            if len(id_data) < 2:
                rsi_values.extend([np.nan] * len(id_data))
                continue
            
            pct_changes = np.diff(id_data) / id_data[:-1]
            gains = np.where(pct_changes > 0, pct_changes, 0)
            losses = np.where(pct_changes < 0, -pct_changes, 0)
            
            avg_gains = []
            avg_losses = []
            
            for i in range(len(pct_changes)):
                start_idx = max(0, i - window + 1)
                avg_gains.append(np.mean(gains[start_idx:i+1]))
                avg_losses.append(np.mean(losses[start_idx:i+1]))
            
            rsi = []
            for i in range(len(avg_gains)):
                if avg_losses[i] == 0:
                    rsi.append(100)
                else:
                    rs = avg_gains[i] / avg_losses[i]
                    rsi.append(100 - (100 / (1 + rs)))
            
            rsi_values.extend([np.nan] + rsi)
        
        return pd.Series(rsi_values, index=self.long_df.index)
    
    def create_enhanced_features(self):
        """åˆ›å»ºå¢å¼ºç‰¹å¾ç»„åˆ"""
        print("æ­£åœ¨åˆ›å»ºå¢å¼ºç‰¹å¾ç»„åˆ...")
        
        feature_cols = [col for col in self.merged_df.columns 
                       if col not in ['id', 'time', 'target', 'window']]
        
        X = self.merged_df[feature_cols].fillna(0)
        y = self.merged_df['target'].fillna(0)
        
        # ç§»é™¤åŒ…å«NaNçš„è¡Œ
        valid_mask = ~(X.isna().any(axis=1) | y.isna())
        X = X[valid_mask]
        y = y[valid_mask]
        
        print(f"æ•°æ®æ¸…ç†å: X={X.shape}, y={y.shape}")
        
        # åˆ›å»ºå¢å¼ºç‰¹å¾
        enhanced_features = {}
        
        # 1. ä»·æ ¼ç›¸å…³ç‰¹å¾ç»„åˆ
        if 'mean' in X.columns and 'std' in X.columns:
            enhanced_features['price_volatility_ratio'] = X['mean'] / (X['std'] + 1e-8)
            enhanced_features['price_range_ratio'] = (X['max'] - X['min']) / (X['mean'] + 1e-8)
            enhanced_features['price_skewness_abs'] = np.abs(X['skew'])
            enhanced_features['price_kurtosis_norm'] = X['kurt'] / (X['std'] + 1e-8)
        
        # 2. ç§»åŠ¨å¹³å‡ç›¸å…³ç‰¹å¾
        if 'sma' in X.columns and 'ema' in X.columns:
            enhanced_features['trend_strength'] = (X['sma'] - X['ema']) / (X['ema'] + 1e-8)
            enhanced_features['trend_consistency'] = np.abs(X['sma'] - X['ema']) / (X['std'] + 1e-8)
            enhanced_features['ma_cross_signal'] = np.where(X['sma'] > X['ema'], 1, -1)
        
        # 3. RSIç›¸å…³ç‰¹å¾
        if 'rsi' in X.columns:
            enhanced_features['rsi_signal'] = np.where(X['rsi'] > 70, 1, np.where(X['rsi'] < 30, -1, 0))
            enhanced_features['rsi_distance'] = np.abs(X['rsi'] - 50) / 50
            enhanced_features['rsi_trend'] = np.where(X['rsi'] > 50, 1, -1)
        
        # 4. åˆ†ä½æ•°ç›¸å…³ç‰¹å¾
        if 'q25' in X.columns and 'q75' in X.columns:
            enhanced_features['iqr_ratio'] = (X['q75'] - X['q25']) / (X['mean'] + 1e-8)
            enhanced_features['price_position'] = (X['mean'] - X['q25']) / (X['q75'] - X['q25'] + 1e-8)
        
        # 5. ä»·æ ¼å˜åŒ–ç›¸å…³ç‰¹å¾
        if 'pct_change' in X.columns and 'pct_change_rolling' in X.columns:
            enhanced_features['momentum_strength'] = X['pct_change'] / (X['pct_change_rolling'] + 1e-8)
            enhanced_features['momentum_consistency'] = np.abs(X['pct_change'] - X['pct_change_rolling'])
        
        # 6. å¤åˆç‰¹å¾
        if 'mean' in X.columns and 'std' in X.columns and 'rsi' in X.columns:
            enhanced_features['volatility_rsi_interaction'] = X['std'] * (X['rsi'] / 100)
            enhanced_features['mean_rsi_ratio'] = X['mean'] * (X['rsi'] / 100)
        
        if 'sma' in X.columns and 'ema' in X.columns and 'rsi' in X.columns:
            enhanced_features['trend_rsi_signal'] = ((X['sma'] - X['ema']) / (X['ema'] + 1e-8)) * (X['rsi'] / 100)
        
        # 7. ç»Ÿè®¡ç‰¹å¾ç»„åˆ
        if 'skew' in X.columns and 'kurt' in X.columns:
            enhanced_features['distribution_score'] = np.abs(X['skew']) + np.abs(X['kurt'])
            enhanced_features['skew_kurt_ratio'] = X['skew'] / (X['kurt'] + 1e-8)
        
        # 8. ä»·æ ¼ä½ç½®ç‰¹å¾
        if 'min' in X.columns and 'max' in X.columns and 'mean' in X.columns:
            enhanced_features['price_position_relative'] = (X['mean'] - X['min']) / (X['max'] - X['min'] + 1e-8)
            enhanced_features['price_range_efficiency'] = (X['max'] - X['min']) / (X['std'] + 1e-8)
        
        # åˆ›å»ºç‰¹å¾DataFrame
        if enhanced_features:
            self.features = pd.DataFrame(enhanced_features)
            self.feature_names = list(enhanced_features.keys())
        else:
            self.features = X
            self.feature_names = feature_cols
        
        print(f"åˆ›å»ºäº† {len(self.feature_names)} ä¸ªå¢å¼ºç‰¹å¾")
        
        # æ‰“å°ç‰¹å¾è¡¨è¾¾å¼
        self._print_enhanced_features(enhanced_features)
        
        return self
    
    def _print_enhanced_features(self, enhanced_features):
        """æ‰“å°å¢å¼ºç‰¹å¾è¡¨è¾¾å¼"""
        print("\n" + "="*60)
        print("ğŸ¯ å¢å¼ºç‰¹å¾è¡¨è¾¾å¼")
        print("="*60)
        
        for i, (name, expression) in enumerate(enhanced_features.items(), 1):
            print(f"ç‰¹å¾{i}: {name}")
            print(f"   è¡¨è¾¾å¼: {expression}")
            print()
        
        print("="*60)
    
    def calculate_ic(self):
        """è®¡ç®—ç‰¹å¾ä¸ç›®æ ‡å˜é‡çš„ä¿¡æ¯ç³»æ•°"""
        print("æ­£åœ¨è®¡ç®—ICå€¼...")
        
        if self.features is None:
            print("è¯·å…ˆåˆ›å»ºç‰¹å¾ï¼")
            return None
        
        # åˆå¹¶ç‰¹å¾å’Œç›®æ ‡
        ic_data = pd.concat([
            self.features,
            self.merged_df[['id', 'time', 'target']]
        ], axis=1)
        
        # æŒ‰æ—¶é—´å’ŒIDåˆ†ç»„è®¡ç®—IC
        ic_results = []
        
        for time_group in ic_data.groupby('time'):
            time = time_group[0]
            group_data = time_group[1]
            
            for feature in self.feature_names:
                if feature in group_data.columns:
                    try:
                        # è®¡ç®—ç›¸å…³ç³»æ•°
                        corr = group_data[feature].corr(group_data['target'])
                        
                        # è®¡ç®—äº’ä¿¡æ¯
                        mi = mutual_info_score(
                            group_data[feature].fillna(0),
                            group_data['target']
                        )
                        
                        ic_results.append({
                            'time': time,
                            'feature': feature,
                            'correlation': corr,
                            'mutual_info': mi,
                            'ic_score': abs(corr) if not pd.isna(corr) else 0
                        })
                    except Exception as e:
                        print(f"è­¦å‘Šï¼šç‰¹å¾ {feature} åœ¨æ—¶é—´ {time} çš„ICè®¡ç®—å¤±è´¥: {e}")
                        continue
        
        self.ic_df = pd.DataFrame(ic_results)
        print(f"ICè®¡ç®—å®Œæˆï¼Œå…± {len(ic_results)} æ¡è®°å½•")
        return self.ic_df
    
    def analyze_ic(self):
        """åˆ†æICç»“æœ"""
        if not hasattr(self, 'ic_df') or self.ic_df is None:
            print("è¯·å…ˆè®¡ç®—ICå€¼ï¼")
            return None
        
        print("æ­£åœ¨åˆ†æICç»“æœ...")
        
        # è®¡ç®—ç‰¹å¾çš„å¹³å‡ICå€¼
        feature_ic = self.ic_df.groupby('feature').agg({
            'ic_score': ['mean', 'std', 'count'],
            'correlation': ['mean', 'std'],
            'mutual_info': ['mean', 'std']
        }).round(4)
        
        feature_ic.columns = [
            'IC_mean', 'IC_std', 'IC_count',
            'Corr_mean', 'Corr_std',
            'MI_mean', 'MI_std'
        ]
        
        # æŒ‰å¹³å‡ICå€¼æ’åº
        feature_ic = feature_ic.sort_values('IC_mean', ascending=False)
        
        self.feature_ic_summary = feature_ic
        print("ICåˆ†æå®Œæˆ")
        return feature_ic
    
    def visualize_results(self):
        """å¯è§†åŒ–ç»“æœ"""
        if not hasattr(self, 'feature_ic_summary') or self.feature_ic_summary is None:
            print("è¯·å…ˆåˆ†æICç»“æœï¼")
            return
        
        if not hasattr(self, 'ic_df') or self.ic_df is None or len(self.ic_df) == 0:
            print("æ²¡æœ‰ICæ•°æ®å¯ä¾›å¯è§†åŒ–ï¼")
            return
        
        print("æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        try:
            # åˆ›å»ºå›¾è¡¨
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('å¢å¼ºç‰¹å¾æå–ç»“æœåˆ†æ', fontsize=16)
            
            # 1. ç‰¹å¾ICå€¼æ’å
            top_features = self.feature_ic_summary.head(20)
            if len(top_features) > 0:
                axes[0, 0].barh(range(len(top_features)), top_features['IC_mean'])
                axes[0, 0].set_yticks(range(len(top_features)))
                axes[0, 0].set_yticklabels(top_features.index)
                axes[0, 0].set_xlabel('å¹³å‡ICå€¼')
                axes[0, 0].set_title('Top 20ç‰¹å¾ICå€¼æ’å')
                axes[0, 0].invert_yaxis()
            else:
                axes[0, 0].text(0.5, 0.5, 'æ— æ•°æ®', ha='center', va='center', transform=axes[0, 0].transAxes)
                axes[0, 0].set_title('Top 20ç‰¹å¾ICå€¼æ’å')
            
            # 2. ICå€¼åˆ†å¸ƒ
            if len(self.ic_df) > 0:
                axes[0, 1].hist(self.ic_df['ic_score'], bins=30, alpha=0.7, edgecolor='black')
                axes[0, 1].set_xlabel('ICå€¼')
                axes[0, 1].set_ylabel('é¢‘æ¬¡')
                axes[0, 1].set_title('ICå€¼åˆ†å¸ƒ')
            else:
                axes[0, 1].text(0.5, 0.5, 'æ— æ•°æ®', ha='center', va='center', transform=axes[0, 1].transAxes)
                axes[0, 1].set_title('ICå€¼åˆ†å¸ƒ')
            
            # 3. ç›¸å…³æ€§vsäº’ä¿¡æ¯æ•£ç‚¹å›¾
            if len(self.ic_df) > 0:
                axes[1, 0].scatter(
                    self.ic_df['correlation'].abs(),
                    self.ic_df['mutual_info'],
                    alpha=0.6
                )
                axes[1, 0].set_xlabel('|ç›¸å…³ç³»æ•°|')
                axes[1, 0].set_ylabel('äº’ä¿¡æ¯')
                axes[1, 0].set_title('ç›¸å…³æ€§ vs äº’ä¿¡æ¯')
            else:
                axes[1, 0].text(0.5, 0.5, 'æ— æ•°æ®', ha='center', va='center', transform=axes[1, 0].transAxes)
                axes[1, 0].set_title('ç›¸å…³æ€§ vs äº’ä¿¡æ¯')
            
            # 4. æ—¶é—´åºåˆ—ICå€¼
            if len(self.ic_df) > 0:
                time_ic = self.ic_df.groupby('time')['ic_score'].mean()
                if len(time_ic) > 0:
                    axes[1, 1].plot(time_ic.index, time_ic.values, marker='o')
                    axes[1, 1].set_xlabel('æ—¶é—´')
                    axes[1, 1].set_ylabel('å¹³å‡ICå€¼')
                    axes[1, 1].set_title('æ—¶é—´åºåˆ—ICå€¼å˜åŒ–')
                    axes[1, 1].tick_params(axis='x', rotation=45)
                else:
                    axes[1, 1].text(0.5, 0.5, 'æ— æ•°æ®', ha='center', va='center', transform=axes[1, 1].transAxes)
                    axes[1, 1].set_title('æ—¶é—´åºåˆ—ICå€¼å˜åŒ–')
            else:
                axes[1, 1].text(0.5, 0.5, 'æ— æ•°æ®', ha='center', va='center', transform=axes[1, 1].transAxes)
                axes[1, 1].set_title('æ—¶é—´åºåˆ—ICå€¼å˜åŒ–')
            
            plt.tight_layout()
            plt.savefig('source/å¯è§†åŒ–æ–‡ä»¶/enhanced_ic_analysis.png', dpi=300, bbox_inches='tight')
            plt.show()
            
            print("å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ° source/å¯è§†åŒ–æ–‡ä»¶/enhanced_ic_analysis.png")
            
        except Exception as e:
            print(f"å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        print("æ­£åœ¨ä¿å­˜ç»“æœ...")
        
        # ä¿å­˜ç‰¹å¾
        if self.features is not None:
            self.features.to_csv('source/ç»“æœæ–‡ä»¶/enhanced_features.csv', index=False)
            print("ç‰¹å¾å·²ä¿å­˜åˆ°: source/ç»“æœæ–‡ä»¶/enhanced_features.csv")
        
        # ä¿å­˜ICç»“æœ
        if hasattr(self, 'ic_df') and self.ic_df is not None:
            self.ic_df.to_csv('source/ç»“æœæ–‡ä»¶/enhanced_ic_results.csv', index=False)
            print("ICç»“æœå·²ä¿å­˜åˆ°: source/ç»“æœæ–‡ä»¶/enhanced_ic_results.csv")
        
        # ä¿å­˜ICåˆ†ææ‘˜è¦
        if hasattr(self, 'feature_ic_summary') and self.feature_ic_summary is not None:
            self.feature_ic_summary.to_csv('source/ç»“æœæ–‡ä»¶/enhanced_ic_summary.csv')
            print("ICåˆ†ææ‘˜è¦å·²ä¿å­˜åˆ°: source/ç»“æœæ–‡ä»¶/enhanced_ic_summary.csv")
        
        print("æ‰€æœ‰ç»“æœä¿å­˜å®Œæˆï¼")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("å¢å¼ºç‰ˆç‰¹å¾æå–å’ŒICæµ‹è¯•ç¨‹åº")
    print("=" * 60)
    
    # åˆ›å»ºç‰¹å¾æå–å™¨
    extractor = EnhancedFeatureExtractor()
    
    # åŠ è½½æ•°æ®
    extractor.load_data(
        target_file='source/æ•°æ®æ–‡ä»¶/tsfresh_target_panel.csv',
        long_file='source/æ•°æ®æ–‡ä»¶/tsfresh_long.csv',
        target_id='000001.SZ'
    )
    
    # å‡†å¤‡ç‰¹å¾
    extractor.prepare_features(window_sizes=[5, 10])
    
    # åˆ›å»ºå¢å¼ºç‰¹å¾
    extractor.create_enhanced_features()
    
    # è®¡ç®—ICå€¼
    extractor.calculate_ic()
    
    # åˆ†æICç»“æœ
    extractor.analyze_ic()
    
    # å¯è§†åŒ–ç»“æœ
    extractor.visualize_results()
    
    # ä¿å­˜ç»“æœ
    extractor.save_results()
    
    print("\nç¨‹åºæ‰§è¡Œå®Œæˆï¼")


if __name__ == "__main__":
    main()
