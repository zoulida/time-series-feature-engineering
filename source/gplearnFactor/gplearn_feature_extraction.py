#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨gplearnè¿›è¡Œæ—¶åºç‰¹å¾æå–å’ŒICæµ‹è¯•
ä½œè€…ï¼šAIåŠ©æ‰‹
æ—¥æœŸï¼š2024å¹´
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mutual_info_score
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

class GPLearnFeatureExtractor:
    """ä½¿ç”¨gplearnè¿›è¡Œç‰¹å¾æå–çš„ç±»"""
    
    def __init__(self):
        self.features = None
        self.feature_names = None
        self.scaler = StandardScaler()
        
    def load_data(self, target_file, long_file, target_id='000001.SZ'):
        """åŠ è½½æ•°æ®æ–‡ä»¶"""
        print("æ­£åœ¨åŠ è½½æ•°æ®...")
        
        # åŠ è½½ç›®æ ‡æ•°æ®
        self.target_df = pd.read_csv(target_file)
        self.target_df['time'] = pd.to_datetime(self.target_df['time'])
        
        # åŠ è½½æ—¶åºæ•°æ®
        self.long_df = pd.read_csv(long_file)
        self.long_df['time'] = pd.to_datetime(self.long_df['time'])
        
        # åªä¿ç•™æŒ‡å®šIDçš„æ•°æ®
        self.target_df = self.target_df[self.target_df['id'] == target_id].copy()
        self.long_df = self.long_df[self.long_df['id'] == target_id].copy()
        
        print(f"ç›®æ ‡æ•°æ®å½¢çŠ¶: {self.target_df.shape}")
        print(f"æ—¶åºæ•°æ®å½¢çŠ¶: {self.long_df.shape}")
        print(f"å¤„ç†ç›®æ ‡ID: {target_id}")
        
        return self
    
    def prepare_features(self, window_sizes=[5, 10, 20]):
        """å‡†å¤‡æ—¶åºç‰¹å¾"""
        print("æ­£åœ¨å‡†å¤‡æ—¶åºç‰¹å¾...")
        
        feature_dfs = []
        total_windows = len(window_sizes)
        
        for i, window in enumerate(window_sizes, 1):
            print(f"å¤„ç†çª—å£å¤§å°: {window} ({i}/{total_windows})")
            
            # è®¡ç®—æ»šåŠ¨ç»Ÿè®¡é‡ - ä¿æŒç´¢å¼•ç»“æ„
            rolling_stats = self.long_df.groupby('id')['value'].rolling(
                window=window, min_periods=1
            ).agg([
                'mean', 'std', 'min', 'max', 'median',
                'skew', 'kurt', 'var'
            ])
            
            # å•ç‹¬è®¡ç®—åˆ†ä½æ•°
            q25 = self.long_df.groupby('id')['value'].rolling(
                window=window, min_periods=1
            ).quantile(0.25)
            
            q75 = self.long_df.groupby('id')['value'].rolling(
                window=window, min_periods=1
            ).quantile(0.75)
            
            # è®¡ç®—ä»·æ ¼å˜åŒ–
            pct_change = self.long_df.groupby('id')['value'].pct_change()
            pct_change_rolling = self.long_df.groupby('id')['value'].pct_change().rolling(
                window=window, min_periods=1
            ).mean()
            
            # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
            sma = self.long_df.groupby('id')['value'].rolling(
                window=window, min_periods=1
            ).mean()
            
            ema = self.long_df.groupby('id')['value'].ewm(
                span=window, min_periods=1
            ).mean()
            
            rsi = self._calculate_rsi(window)
            
            # åˆ›å»ºç‰¹å¾DataFrameï¼Œä¿æŒæ­£ç¡®çš„ç´¢å¼•ç»“æ„
            rolling_features = pd.DataFrame({
                'id': self.long_df['id'],
                'time': self.long_df['time'],
                'mean': rolling_stats['mean'].reset_index(0, drop=True),
                'std': rolling_stats['std'].reset_index(0, drop=True),
                'min': rolling_stats['min'].reset_index(0, drop=True),
                'max': rolling_stats['max'].reset_index(0, drop=True),
                'median': rolling_stats['median'].reset_index(0, drop=True),
                'skew': rolling_stats['skew'].reset_index(0, drop=True),
                'kurt': rolling_stats['kurt'].reset_index(0, drop=True),
                'var': rolling_stats['var'].reset_index(0, drop=True),
                'q25': q25.reset_index(0, drop=True),
                'q75': q75.reset_index(0, drop=True),
                'pct_change': pct_change.reset_index(0, drop=True),
                'pct_change_rolling': pct_change_rolling.reset_index(0, drop=True),
                'sma': sma.reset_index(0, drop=True),
                'ema': ema.reset_index(0, drop=True),
                'rsi': rsi,
                'window': window
            })
            
            feature_dfs.append(rolling_features)
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        self.features_df = pd.concat(feature_dfs, ignore_index=True)
        
        # ç¡®ä¿æ—¶é—´åˆ—æ ¼å¼ä¸€è‡´
        self.features_df['time'] = pd.to_datetime(self.features_df['time'])
        
        # ä¸ç›®æ ‡æ•°æ®åˆå¹¶
        self.merged_df = pd.merge(
            self.target_df,
            self.features_df,
            on=['id', 'time'],
            how='left'
        )
        
        print(f"åˆå¹¶åæ•°æ®å½¢çŠ¶: {self.merged_df.shape}")
        return self
    
    def _calculate_rsi(self, window):
        """è®¡ç®—RSIæŒ‡æ ‡"""
        # ä½¿ç”¨æ›´ç®€å•çš„æ–¹æ³•ï¼Œé¿å…å¤æ‚çš„ç´¢å¼•æ“ä½œ
        rsi_values = []
        
        for id_val in self.long_df['id'].unique():
            # è·å–å½“å‰IDçš„æ•°æ®
            id_data = self.long_df[self.long_df['id'] == id_val]['value'].values
            
            if len(id_data) < 2:
                # å¦‚æœæ•°æ®ä¸è¶³ï¼Œå¡«å……NaN
                rsi_values.extend([np.nan] * len(id_data))
                continue
            
            # è®¡ç®—ç™¾åˆ†æ¯”å˜åŒ–
            pct_changes = np.diff(id_data) / id_data[:-1]
            
            # è®¡ç®—å¢ç›Šå’ŒæŸå¤±
            gains = np.where(pct_changes > 0, pct_changes, 0)
            losses = np.where(pct_changes < 0, -pct_changes, 0)
            
            # è®¡ç®—æ»šåŠ¨å¹³å‡
            avg_gains = []
            avg_losses = []
            
            for i in range(len(pct_changes)):
                start_idx = max(0, i - window + 1)
                avg_gains.append(np.mean(gains[start_idx:i+1]))
                avg_losses.append(np.mean(losses[start_idx:i+1]))
            
            # è®¡ç®—RSI
            rsi = []
            for i in range(len(avg_gains)):
                if avg_losses[i] == 0:
                    rsi.append(100)
                else:
                    rs = avg_gains[i] / avg_losses[i]
                    rsi.append(100 - (100 / (1 + rs)))
            
            # ç¬¬ä¸€ä¸ªå€¼è®¾ä¸ºNaNï¼ˆå› ä¸ºæ²¡æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®ï¼‰
            rsi_values.extend([np.nan] + rsi)
        
        return pd.Series(rsi_values, index=self.long_df.index)
    
    def create_gplearn_features(self, n_features=50, population_size=1000, generations=20):
        """ä½¿ç”¨gplearnåˆ›å»ºç¬¦å·å›å½’ç‰¹å¾"""
        try:
            # åº”ç”¨numpyå…¼å®¹æ€§è¡¥ä¸
            import numpy as np
            if not hasattr(np, 'int'):
                np.int = int
            if not hasattr(np, 'float'):
                np.float = float
            if not hasattr(np, 'bool'):
                np.bool = bool
            
            from gplearn.genetic import SymbolicTransformer
            print("æ­£åœ¨ä½¿ç”¨gplearnåˆ›å»ºç¬¦å·å›å½’ç‰¹å¾...")
            
            # å‡†å¤‡ç‰¹å¾çŸ©é˜µ
            feature_cols = [col for col in self.merged_df.columns 
                          if col not in ['id', 'time', 'target', 'window']]
            
            X = self.merged_df[feature_cols].fillna(0)
            y = self.merged_df['target'].fillna(0)
            
            # ç§»é™¤åŒ…å«NaNçš„è¡Œ
            print(f"æ•°æ®æ¸…ç†å‰: X={X.shape}, y={y.shape}")
            print(f"æ¸…ç†å‰NaNç»Ÿè®¡: Xä¸­æœ‰NaNçš„è¡Œæ•°={X.isna().any(axis=1).sum()}, yä¸­æœ‰NaNçš„è¡Œæ•°={y.isna().sum()}")
            
            valid_mask = ~(X.isna().any(axis=1) | y.isna())
            X = X[valid_mask]
            y = y[valid_mask]
            
            print(f"æ•°æ®æ¸…ç†å: X={X.shape}, y={y.shape}")
            print(f"æ¸…ç†åNaNæ£€æŸ¥: Xä¸­æœ‰NaN={X.isna().any().any()}, yä¸­æœ‰NaN={y.isna().any()}")
            
            # æ‰“å°æ¸…ç†åçš„æ•°æ®æ ·æœ¬
            print("\næ•°æ®æ¸…ç†åçš„æ ·æœ¬:")
            print("Xçš„å‰5è¡Œ:")
            print(X.head())
            print("\nyçš„å‰10ä¸ªå€¼:")
            print(y.head(10))
            print(f"\nXçš„åˆ—å: {list(X.columns)}")
            
            # ä¿å­˜æ•°æ®æ¸…ç†åçš„ä¸­é—´æ•°æ®
            print("\nä¿å­˜æ•°æ®æ¸…ç†åçš„ä¸­é—´æ•°æ®...")
            X.to_csv('source/ç»“æœæ–‡ä»¶/cleaned_X_data.csv', index=False)
            y.to_csv('source/ç»“æœæ–‡ä»¶/cleaned_y_data.csv', index=False)
            print("âœ… ä¸­é—´æ•°æ®å·²ä¿å­˜:")
            print("   - Xæ•°æ®: source/ç»“æœæ–‡ä»¶/cleaned_X_data.csv")
            print("   - yæ•°æ®: source/ç»“æœæ–‡ä»¶/cleaned_y_data.csv")
            
            # æ ‡å‡†åŒ–ç‰¹å¾
            X_scaled = self.scaler.fit_transform(X)
            
            # åˆ›å»ºç¬¦å·å›å½’è½¬æ¢å™¨ - ä¿®å¤å‚æ•°é…ç½®
            gp_transformer = SymbolicTransformer(
                population_size=100,     # å¢åŠ ç§ç¾¤å¤§å°
                generations=5,           # å‡å°‘ä»£æ•°
                n_components=5,          # å‡å°‘ç‰¹å¾æ•°
                function_set=['add', 'sub', 'mul', 'div'],  # ç®€åŒ–å‡½æ•°é›†
                metric='spearman',
                random_state=42,
                n_jobs=1,
                hall_of_fame=50,         # ç¡®ä¿hall_of_fame <= population_size
                parsimony_coefficient=0.001  # æ·»åŠ ç®€çº¦ç³»æ•°
            )
            
            # æ‹Ÿåˆè½¬æ¢å™¨ - ä¿®å¤ç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
            try:
                # å°è¯•ç›´æ¥ä½¿ç”¨fit_transform
                X_transformed = gp_transformer.fit_transform(X_scaled, y)
            except Exception as e:
                print(f"fit_transformå¤±è´¥ï¼Œå°è¯•fit+transform: {e}")
                try:
                    # åˆ†åˆ«è°ƒç”¨fitå’Œtransform
                    gp_transformer.fit(X_scaled, y)
                    X_transformed = gp_transformer.transform(X_scaled)
                except Exception as e2:
                    print(f"fit+transformä¹Ÿå¤±è´¥: {e2}")
                    raise e2
            
            # è·å–ç‰¹å¾åç§°
            self.feature_names = [f"GP_Feature_{i}" for i in range(X_transformed.shape[1])]
            
            # åˆ›å»ºç‰¹å¾DataFrame
            self.features = pd.DataFrame(
                X_transformed,
                columns=self.feature_names,
                index=self.merged_df.index
            )
            
            print(f"æˆåŠŸåˆ›å»º {len(self.feature_names)} ä¸ªgplearnç‰¹å¾")
            
            # ğŸ†• æ‰“å°ç”Ÿæˆçš„ç‰¹å¾è¡¨è¾¾å¼
            self._print_generated_features(gp_transformer, feature_cols)
            
            # ğŸ†• æ¨æ–­å¹¶æ‰“å°GPLearnç‰¹å¾è¡¨è¾¾å¼
            self._infer_and_print_feature_expressions(X, feature_cols)
            
            # æ‰“å°ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
            print(f"\nğŸ“Š GPLearnç‰¹å¾ç»Ÿè®¡ä¿¡æ¯:")
            print(f"   ç‰¹å¾æ•°é‡: {len(self.feature_names)}")
            print(f"   æ•°æ®å½¢çŠ¶: {self.features.shape}")
            print(f"   ç‰¹å¾åç§°: {self.feature_names}")
            
            # æ‰“å°ç‰¹å¾æ•°æ®æ ·æœ¬
            print(f"\nğŸ“ˆ GPLearnç‰¹å¾æ•°æ®æ ·æœ¬ (å‰5è¡Œ):")
            print(self.features.head())
            
            # æ‰“å°ç‰¹å¾ç»Ÿè®¡æè¿°
            print(f"\nğŸ“‹ GPLearnç‰¹å¾ç»Ÿè®¡æè¿°:")
            print(self.features.describe())
            
            return self
            
        except ImportError:
            print("gplearnæœªå®‰è£…ï¼Œä½¿ç”¨åŸºç¡€ç‰¹å¾æ›¿ä»£...")
            return self._create_basic_features()
        except Exception as e:
            print(f"gplearnç‰¹å¾åˆ›å»ºå¤±è´¥: {e}")
            print("ä½¿ç”¨åŸºç¡€ç‰¹å¾æ›¿ä»£...")
            return self._create_basic_features()
    
    def _print_generated_features(self, gp_transformer, original_features):
        """æ‰“å°ç”Ÿæˆçš„ç‰¹å¾è¡¨è¾¾å¼"""
        try:
            print("\n" + "="*60)
            print("ğŸ¯ GPLearn ç”Ÿæˆçš„ç‰¹å¾è¡¨è¾¾å¼")
            print("="*60)
            
            # å°è¯•å¤šç§æ–¹å¼è·å–ç‰¹å¾è¡¨è¾¾å¼
            print("å°è¯•è·å–ç‰¹å¾è¡¨è¾¾å¼...")
            
            # æ–¹æ³•1: æ£€æŸ¥best_programs_
            if hasattr(gp_transformer, 'best_programs_') and gp_transformer.best_programs_ is not None:
                print(f"âœ… æ‰¾åˆ° best_programs_ï¼ŒåŒ…å« {len(gp_transformer.best_programs_)} ä¸ªç¨‹åº")
                for i, program in enumerate(gp_transformer.best_programs_):
                    if i < len(self.feature_names):
                        feature_name = self.feature_names[i]
                        expression = str(program)
                        # æ›¿æ¢ç‰¹å¾ç´¢å¼•ä¸ºå®é™…ç‰¹å¾å
                        for j, feat in enumerate(original_features):
                            expression = expression.replace(f'X[:, {j}]', feat)
                        print(f"{feature_name}: {expression}")
                        if hasattr(program, 'raw_fitness_'):
                            print(f"   é€‚åº”åº¦åˆ†æ•°: {program.raw_fitness_:.6f}")
                        print()
            
            # æ–¹æ³•2: æ£€æŸ¥programs_
            elif hasattr(gp_transformer, 'programs_') and gp_transformer.programs_ is not None:
                print(f"âœ… æ‰¾åˆ° programs_ï¼ŒåŒ…å« {len(gp_transformer.programs_)} ä¸ªç¨‹åº")
                for i, program in enumerate(gp_transformer.programs_):
                    if i < len(self.feature_names):
                        feature_name = self.feature_names[i]
                        expression = str(program)
                        for j, feat in enumerate(original_features):
                            expression = expression.replace(f'X[:, {j}]', feat)
                        print(f"{feature_name}: {expression}")
                        print()
            
            # æ–¹æ³•3: æ£€æŸ¥å…¶ä»–å¯èƒ½çš„å±æ€§
            else:
                print("âš ï¸  æ— æ³•é€šè¿‡å¸¸è§„æ–¹æ³•è·å–ç‰¹å¾è¡¨è¾¾å¼")
                print("æ£€æŸ¥æ‰€æœ‰å¯ç”¨å±æ€§...")
                
                # åˆ—å‡ºæ‰€æœ‰å±æ€§
                all_attrs = [attr for attr in dir(gp_transformer) if not attr.startswith('_')]
                print(f"å¯ç”¨å±æ€§: {all_attrs}")
                
                # å°è¯•æ£€æŸ¥ä¸€äº›å¯èƒ½çš„å±æ€§
                for attr in ['programs', 'best_programs', 'fitted_programs', 'final_programs']:
                    if hasattr(gp_transformer, attr):
                        value = getattr(gp_transformer, attr)
                        print(f"å±æ€§ {attr}: {type(value)}, é•¿åº¦: {len(value) if hasattr(value, '__len__') else 'N/A'}")
                
                # å°è¯•é€šè¿‡transformerå±æ€§è·å–
                if hasattr(gp_transformer, 'transformer'):
                    print("æ£€æŸ¥transformerå±æ€§...")
                    transformer = gp_transformer.transformer
                    if hasattr(transformer, 'best_programs_'):
                        print(f"transformer.best_programs_ å­˜åœ¨ï¼Œé•¿åº¦: {len(transformer.best_programs_)}")
                        for i, program in enumerate(transformer.best_programs_):
                            if i < len(self.feature_names):
                                feature_name = self.feature_names[i]
                                expression = str(program)
                                for j, feat in enumerate(original_features):
                                    expression = expression.replace(f'X[:, {j}]', feat)
                                print(f"{feature_name}: {expression}")
                                print()
                
                print("ç‰¹å¾å·²ä¿å­˜åˆ°CSVæ–‡ä»¶ä¸­")
            
            print("="*60)
            
        except Exception as e:
            print(f"æ‰“å°ç‰¹å¾è¡¨è¾¾å¼æ—¶å‡ºé”™: {e}")
            print("ä½†ç‰¹å¾æ•°æ®å·²æˆåŠŸåˆ›å»º")
            import traceback
            traceback.print_exc()
     
     def _infer_and_print_feature_expressions(self, X, original_features):
         """æ¨æ–­å¹¶æ‰“å°GPLearnç‰¹å¾è¡¨è¾¾å¼"""
         try:
             print("\n" + "="*60)
             print("ğŸ¯ GPLearnç‰¹å¾è¡¨è¾¾å¼æ¨æ–­")
             print("="*60)
             
             from sklearn.linear_model import LinearRegression
             import numpy as np
             
             # æ ‡å‡†åŒ–åŸå§‹ç‰¹å¾
             X_scaled = self.scaler.transform(X)
             X_scaled_df = pd.DataFrame(X_scaled, columns=original_features)
             
             print("ğŸ“Š åŸå§‹ç‰¹å¾åˆ—è¡¨:")
             for i, col in enumerate(original_features):
                 print(f"  {i:2d}: {col}")
             
             print(f"\nğŸ¯ æ¨æ–­GPLearnç‰¹å¾è¡¨è¾¾å¼:")
             
             # åˆ†ææ¯ä¸ªGPLearnç‰¹å¾
             for i, gp_col in enumerate(self.feature_names):
                 print(f"\nğŸ“ˆ {gp_col}:")
                 
                 # è·å–å½“å‰GPLearnç‰¹å¾
                 gp_feature = self.features[gp_col].values
                 
                 # è®¡ç®—ä¸åŸå§‹ç‰¹å¾çš„ç›¸å…³æ€§
                 correlations = []
                 for j, orig_col in enumerate(X_scaled_df.columns):
                     corr = np.corrcoef(X_scaled_df[orig_col], gp_feature)[0, 1]
                     if not np.isnan(corr):
                         correlations.append((j, orig_col, corr))
                 
                 # æŒ‰ç›¸å…³æ€§ç»å¯¹å€¼æ’åº
                 correlations.sort(key=lambda x: abs(x[2]), reverse=True)
                 
                 print(f"  ä¸åŸå§‹ç‰¹å¾çš„ç›¸å…³æ€§ (Top 3):")
                 for j, (idx, col, corr) in enumerate(correlations[:3]):
                     print(f"    {idx:2d}: {col:15s} -> {corr:8.4f}")
                 
                 # å°è¯•çº¿æ€§å›å½’æ‹Ÿåˆ
                 try:
                     # ä½¿ç”¨ç›¸å…³æ€§æœ€é«˜çš„ç‰¹å¾è¿›è¡Œæ‹Ÿåˆ
                     top_features = [X_scaled_df[corr[1]] for corr in correlations[:3]]
                     X_fit = np.column_stack(top_features)
                     
                     lr = LinearRegression()
                     lr.fit(X_fit, gp_feature)
                     
                     # æ¨æ–­å¯èƒ½çš„è¡¨è¾¾å¼
                     if lr.score(X_fit, gp_feature) > 0.5:
                         print(f"  ğŸ¯ æ¨æ–­è¡¨è¾¾å¼:")
                         expr_parts = []
                         for j, (idx, col, corr) in enumerate(correlations[:3]):
                             coef = lr.coef_[j]
                             if abs(coef) > 0.01:  # åªæ˜¾ç¤ºé‡è¦ç³»æ•°
                                 if coef > 0:
                                     expr_parts.append(f"+{coef:.4f}*{col}")
                                 else:
                                     expr_parts.append(f"{coef:.4f}*{col}")
                         
                         if lr.intercept_ > 0.01:
                             expr_parts.insert(0, f"+{lr.intercept_:.4f}")
                         elif lr.intercept_ < -0.01:
                             expr_parts.insert(0, f"{lr.intercept_:.4f}")
                         
                         expression = "".join(expr_parts)
                         if expression.startswith("+"):
                             expression = expression[1:]
                         print(f"    {gp_col} â‰ˆ {expression}")
                         print(f"    RÂ²: {lr.score(X_fit, gp_feature):8.4f}")
                     else:
                         print(f"  âš ï¸  æ‹Ÿåˆåº¦è¾ƒä½ (RÂ²: {lr.score(X_fit, gp_feature):8.4f})")
                         
                 except Exception as e:
                     print(f"  âŒ æ‹Ÿåˆå¤±è´¥: {e}")
                 
                 # ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
                 print(f"  ç»Ÿè®¡ä¿¡æ¯: å‡å€¼={np.mean(gp_feature):8.4f}, æ ‡å‡†å·®={np.std(gp_feature):8.4f}")
             
             print("\n" + "="*60)
             print("ğŸ“‹ æ¨æ–­å®Œæˆ")
             print("="*60)
             
         except Exception as e:
             print(f"æ¨æ–­ç‰¹å¾è¡¨è¾¾å¼æ—¶å‡ºé”™: {e}")
             import traceback
             traceback.print_exc()
    
    def _create_basic_features(self):
        """åˆ›å»ºåŸºç¡€ç‰¹å¾ï¼ˆå½“gplearnä¸å¯ç”¨æ—¶ï¼‰"""
        print("åˆ›å»ºåŸºç¡€ç‰¹å¾ç»„åˆ...")
        
        feature_cols = [col for col in self.merged_df.columns 
                       if col not in ['id', 'time', 'target', 'window']]
        
        X = self.merged_df[feature_cols].fillna(0)
        
        # åˆ›å»ºä¸€äº›åŸºç¡€ç‰¹å¾ç»„åˆ
        basic_features = {}
        
        # ä»·æ ¼ç›¸å…³ç‰¹å¾ç»„åˆ
        if 'mean' in X.columns and 'std' in X.columns:
            basic_features['price_volatility'] = X['mean'] / (X['std'] + 1e-8)
        
        if 'sma' in X.columns and 'ema' in X.columns:
            basic_features['trend_strength'] = (X['sma'] - X['ema']) / (X['ema'] + 1e-8)
        
        if 'rsi' in X.columns:
            basic_features['rsi_signal'] = np.where(X['rsi'] > 70, 1, np.where(X['rsi'] < 30, -1, 0))
        
        # åˆ›å»ºç‰¹å¾DataFrame
        if basic_features:
            self.features = pd.DataFrame(basic_features)
            self.feature_names = list(basic_features.keys())
        else:
            self.features = X
            self.feature_names = feature_cols
        
        print(f"åˆ›å»ºäº† {len(self.feature_names)} ä¸ªåŸºç¡€ç‰¹å¾")
        return self
    
    def calculate_ic(self):
        """è®¡ç®—ç‰¹å¾ä¸ç›®æ ‡å˜é‡çš„ä¿¡æ¯ç³»æ•°"""
        print("æ­£åœ¨è®¡ç®—ICå€¼...")
        
        if self.features is None:
            print("è¯·å…ˆåˆ›å»ºç‰¹å¾ï¼")
            return None
        
        # åˆå¹¶ç‰¹å¾å’Œç›®æ ‡
        ic_data = pd.concat([
            self.features,
            self.merged_df[['id', 'time', 'target']]
        ], axis=1)
        
        # æŒ‰æ—¶é—´å’ŒIDåˆ†ç»„è®¡ç®—IC
        ic_results = []
        
        for time_group in ic_data.groupby('time'):
            time = time_group[0]
            group_data = time_group[1]
            
            for feature in self.feature_names:
                if feature in group_data.columns:
                    try:
                        # è®¡ç®—ç›¸å…³ç³»æ•°
                        corr = group_data[feature].corr(group_data['target'])
                        
                        # è®¡ç®—äº’ä¿¡æ¯
                        mi = mutual_info_score(
                            group_data[feature].fillna(0),
                            group_data['target']
                        )
                        
                        ic_results.append({
                            'time': time,
                            'feature': feature,
                            'correlation': corr,
                            'mutual_info': mi,
                            'ic_score': abs(corr) if not pd.isna(corr) else 0
                        })
                    except Exception as e:
                        # å¦‚æœæŸä¸ªç‰¹å¾è®¡ç®—å¤±è´¥ï¼Œè®°å½•é”™è¯¯ä½†ç»§ç»­
                        print(f"è­¦å‘Šï¼šç‰¹å¾ {feature} åœ¨æ—¶é—´ {time} çš„ICè®¡ç®—å¤±è´¥: {e}")
                        continue
        
        self.ic_df = pd.DataFrame(ic_results)
        print(f"ICè®¡ç®—å®Œæˆï¼Œå…± {len(ic_results)} æ¡è®°å½•")
        return self.ic_df
    
    def analyze_ic(self):
        """åˆ†æICç»“æœ"""
        if not hasattr(self, 'ic_df') or self.ic_df is None:
            print("è¯·å…ˆè®¡ç®—ICå€¼ï¼")
            return None
        
        print("æ­£åœ¨åˆ†æICç»“æœ...")
        
        # è®¡ç®—ç‰¹å¾çš„å¹³å‡ICå€¼
        feature_ic = self.ic_df.groupby('feature').agg({
            'ic_score': ['mean', 'std', 'count'],
            'correlation': ['mean', 'std'],
            'mutual_info': ['mean', 'std']
        }).round(4)
        
        feature_ic.columns = [
            'IC_mean', 'IC_std', 'IC_count',
            'Corr_mean', 'Corr_std',
            'MI_mean', 'MI_std'
        ]
        
        # æŒ‰å¹³å‡ICå€¼æ’åº
        feature_ic = feature_ic.sort_values('IC_mean', ascending=False)
        
        self.feature_ic_summary = feature_ic
        print("ICåˆ†æå®Œæˆ")
        return feature_ic
    
    def visualize_results(self):
        """å¯è§†åŒ–ç»“æœ"""
        if not hasattr(self, 'feature_ic_summary') or self.feature_ic_summary is None:
            print("è¯·å…ˆåˆ†æICç»“æœï¼")
            return
        
        if not hasattr(self, 'ic_df') or self.ic_df is None or len(self.ic_df) == 0:
            print("æ²¡æœ‰ICæ•°æ®å¯ä¾›å¯è§†åŒ–ï¼")
            return
        
        print("æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        try:
            # åˆ›å»ºå›¾è¡¨
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('GPLearnç‰¹å¾æå–ç»“æœåˆ†æ', fontsize=16)
            
            # 1. ç‰¹å¾ICå€¼æ’å
            top_features = self.feature_ic_summary.head(20)
            if len(top_features) > 0:
                axes[0, 0].barh(range(len(top_features)), top_features['IC_mean'])
                axes[0, 0].set_yticks(range(len(top_features)))
                axes[0, 0].set_yticklabels(top_features.index)
                axes[0, 0].set_xlabel('å¹³å‡ICå€¼')
                axes[0, 0].set_title('Top 20ç‰¹å¾ICå€¼æ’å')
                axes[0, 0].invert_yaxis()
            else:
                axes[0, 0].text(0.5, 0.5, 'æ— æ•°æ®', ha='center', va='center', transform=axes[0, 0].transAxes)
                axes[0, 0].set_title('Top 20ç‰¹å¾ICå€¼æ’å')
            
            # 2. ICå€¼åˆ†å¸ƒ
            if len(self.ic_df) > 0:
                axes[0, 1].hist(self.ic_df['ic_score'], bins=30, alpha=0.7, edgecolor='black')
                axes[0, 1].set_xlabel('ICå€¼')
                axes[0, 1].set_ylabel('é¢‘æ¬¡')
                axes[0, 1].set_title('ICå€¼åˆ†å¸ƒ')
            else:
                axes[0, 1].text(0.5, 0.5, 'æ— æ•°æ®', ha='center', va='center', transform=axes[0, 1].transAxes)
                axes[0, 1].set_title('ICå€¼åˆ†å¸ƒ')
            
            # 3. ç›¸å…³æ€§vsäº’ä¿¡æ¯æ•£ç‚¹å›¾
            if len(self.ic_df) > 0:
                axes[1, 0].scatter(
                    self.ic_df['correlation'].abs(),
                    self.ic_df['mutual_info'],
                    alpha=0.6
                )
                axes[1, 0].set_xlabel('|ç›¸å…³ç³»æ•°|')
                axes[1, 0].set_ylabel('äº’ä¿¡æ¯')
                axes[1, 0].set_title('ç›¸å…³æ€§ vs äº’ä¿¡æ¯')
            else:
                axes[1, 0].text(0.5, 0.5, 'æ— æ•°æ®', ha='center', va='center', transform=axes[1, 0].transAxes)
                axes[1, 0].set_title('ç›¸å…³æ€§ vs äº’ä¿¡æ¯')
            
            # 4. æ—¶é—´åºåˆ—ICå€¼
            if len(self.ic_df) > 0:
                time_ic = self.ic_df.groupby('time')['ic_score'].mean()
                if len(time_ic) > 0:
                    axes[1, 1].plot(time_ic.index, time_ic.values, marker='o')
                    axes[1, 1].set_xlabel('æ—¶é—´')
                    axes[1, 1].set_ylabel('å¹³å‡ICå€¼')
                    axes[1, 1].set_title('æ—¶é—´åºåˆ—ICå€¼å˜åŒ–')
                    axes[1, 1].tick_params(axis='x', rotation=45)
                else:
                    axes[1, 1].text(0.5, 0.5, 'æ— æ•°æ®', ha='center', va='center', transform=axes[1, 1].transAxes)
                    axes[1, 1].set_title('æ—¶é—´åºåˆ—ICå€¼å˜åŒ–')
            else:
                axes[1, 1].text(0.5, 0.5, 'æ— æ•°æ®', ha='center', va='center', transform=axes[1, 1].transAxes)
                axes[1, 1].set_title('æ—¶é—´åºåˆ—ICå€¼å˜åŒ–')
            
            plt.tight_layout()
            plt.savefig('source/å¯è§†åŒ–æ–‡ä»¶/gplearn_ic_analysis.png', dpi=300, bbox_inches='tight')
            plt.show()
            
            print("å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ° source/å¯è§†åŒ–æ–‡ä»¶/gplearn_ic_analysis.png")
            
        except Exception as e:
            print(f"å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def save_results(self):
        """ä¿å­˜ç»“æœ"""
        print("æ­£åœ¨ä¿å­˜ç»“æœ...")
        
        # ä¿å­˜ç‰¹å¾
        if self.features is not None:
            self.features.to_csv('source/ç»“æœæ–‡ä»¶/gplearn_features.csv', index=False)
            print("ç‰¹å¾å·²ä¿å­˜åˆ°: source/ç»“æœæ–‡ä»¶/gplearn_features.csv")
        
        # ä¿å­˜ICç»“æœ
        if hasattr(self, 'ic_df') and self.ic_df is not None:
            self.ic_df.to_csv('source/ç»“æœæ–‡ä»¶/gplearn_ic_results.csv', index=False)
            print("ICç»“æœå·²ä¿å­˜åˆ°: source/ç»“æœæ–‡ä»¶/gplearn_ic_results.csv")
        
        # ä¿å­˜ICåˆ†ææ‘˜è¦
        if hasattr(self, 'feature_ic_summary') and self.feature_ic_summary is not None:
            self.feature_ic_summary.to_csv('source/ç»“æœæ–‡ä»¶/gplearn_ic_summary.csv')
            print("ICåˆ†ææ‘˜è¦å·²ä¿å­˜åˆ°: source/ç»“æœæ–‡ä»¶/gplearn_ic_summary.csv")
        
        print("æ‰€æœ‰ç»“æœä¿å­˜å®Œæˆï¼")


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("GPLearnæ—¶åºç‰¹å¾æå–å’ŒICæµ‹è¯•ç¨‹åº")
    print("=" * 60)
    
    # åˆ›å»ºç‰¹å¾æå–å™¨
    extractor = GPLearnFeatureExtractor()
    
    # åŠ è½½æ•°æ®
    extractor.load_data(
        target_file='source/æ•°æ®æ–‡ä»¶/tsfresh_target_panel.csv',
        long_file='source/æ•°æ®æ–‡ä»¶/tsfresh_long.csv',
        target_id='000001.SZ'
    )
    
    # å‡†å¤‡ç‰¹å¾ - å‡å°‘çª—å£å¤§å°ä»¥åŠ å¿«é€Ÿåº¦
    extractor.prepare_features(window_sizes=[5, 10])
    
    # åˆ›å»ºgplearnç‰¹å¾ - å‡å°‘å‚æ•°ä»¥åŠ å¿«é€Ÿåº¦
    extractor.create_gplearn_features(
        n_features=20,
        population_size=200,
        generations=10
    )
    
    # è®¡ç®—ICå€¼
    extractor.calculate_ic()
    
    # åˆ†æICç»“æœ
    extractor.analyze_ic()
    
    # å¯è§†åŒ–ç»“æœ
    extractor.visualize_results()
    
    # ä¿å­˜ç»“æœ
    extractor.save_results()
    
    print("\nç¨‹åºæ‰§è¡Œå®Œæˆï¼")


if __name__ == "__main__":
    main()
