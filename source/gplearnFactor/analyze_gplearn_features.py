#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åˆ†æžGPLearnç”Ÿæˆçš„ç‰¹å¾ï¼Œå°è¯•æŽ¨æ–­å¯èƒ½çš„è¡¨è¾¾å¼
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression
import warnings
warnings.filterwarnings('ignore')

def analyze_gplearn_features():
    """åˆ†æžGPLearnç”Ÿæˆçš„ç‰¹å¾"""
    print("="*60)
    print("ðŸ” GPLearnç‰¹å¾è¡¨è¾¾å¼åˆ†æž")
    print("="*60)
    
    # åŠ è½½æ•°æ®
    print("åŠ è½½æ•°æ®...")
    X = pd.read_csv('source/ç»“æžœæ–‡ä»¶/cleaned_X_data.csv')
    y = pd.read_csv('source/ç»“æžœæ–‡ä»¶/cleaned_y_data.csv')
    gp_features = pd.read_csv('source/ç»“æžœæ–‡ä»¶/gplearn_features.csv')
    
    print(f"åŽŸå§‹ç‰¹å¾æ•°é‡: {X.shape[1]}")
    print(f"GPLearnç‰¹å¾æ•°é‡: {gp_features.shape[1]}")
    print(f"æ•°æ®æ ·æœ¬æ•°: {X.shape[0]}")
    
    # æ ‡å‡†åŒ–åŽŸå§‹ç‰¹å¾
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)
    X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)
    
    print("\nðŸ“Š åŽŸå§‹ç‰¹å¾åˆ—è¡¨:")
    for i, col in enumerate(X.columns):
        print(f"  {i:2d}: {col}")
    
    print(f"\nðŸŽ¯ GPLearnç‰¹å¾åˆ—è¡¨:")
    for i, col in enumerate(gp_features.columns):
        print(f"  {i}: {col}")
    
    # åˆ†æžæ¯ä¸ªGPLearnç‰¹å¾ä¸ŽåŽŸå§‹ç‰¹å¾çš„å…³ç³»
    print("\n" + "="*60)
    print("ðŸ” ç‰¹å¾è¡¨è¾¾å¼æŽ¨æ–­åˆ†æž")
    print("="*60)
    
    for i, gp_col in enumerate(gp_features.columns):
        print(f"\nðŸ“ˆ åˆ†æž {gp_col}:")
        
        # èŽ·å–å½“å‰GPLearnç‰¹å¾
        gp_feature = gp_features[gp_col].values
        
        # è®¡ç®—ä¸ŽåŽŸå§‹ç‰¹å¾çš„ç›¸å…³æ€§
        correlations = []
        for j, orig_col in enumerate(X_scaled_df.columns):
            corr = np.corrcoef(X_scaled_df[orig_col], gp_feature)[0, 1]
            if not np.isnan(corr):
                correlations.append((j, orig_col, corr))
        
        # æŒ‰ç›¸å…³æ€§ç»å¯¹å€¼æŽ’åº
        correlations.sort(key=lambda x: abs(x[2]), reverse=True)
        
        print(f"  ä¸ŽåŽŸå§‹ç‰¹å¾çš„ç›¸å…³æ€§ (Top 5):")
        for j, (idx, col, corr) in enumerate(correlations[:5]):
            print(f"    {idx:2d}: {col:15s} -> {corr:8.4f}")
        
        # å°è¯•çº¿æ€§å›žå½’æ‹Ÿåˆ
        print(f"  çº¿æ€§å›žå½’æ‹Ÿåˆåˆ†æž:")
        try:
            # ä½¿ç”¨ç›¸å…³æ€§æœ€é«˜çš„ç‰¹å¾è¿›è¡Œæ‹Ÿåˆ
            top_features = [X_scaled_df[corr[1]] for corr in correlations[:3]]
            X_fit = np.column_stack(top_features)
            
            lr = LinearRegression()
            lr.fit(X_fit, gp_feature)
            
            print(f"    ä½¿ç”¨Top 3ç‰¹å¾æ‹Ÿåˆ:")
            for j, (idx, col, corr) in enumerate(correlations[:3]):
                coef = lr.coef_[j]
                print(f"      {col:15s} * {coef:8.4f}")
            print(f"    æˆªè·: {lr.intercept_:8.4f}")
            print(f"    RÂ²: {lr.score(X_fit, gp_feature):8.4f}")
            
            # æŽ¨æ–­å¯èƒ½çš„è¡¨è¾¾å¼
            if lr.score(X_fit, gp_feature) > 0.5:
                print(f"    ðŸŽ¯ æŽ¨æ–­è¡¨è¾¾å¼:")
                expr_parts = []
                for j, (idx, col, corr) in enumerate(correlations[:3]):
                    coef = lr.coef_[j]
                    if abs(coef) > 0.01:  # åªæ˜¾ç¤ºé‡è¦ç³»æ•°
                        if coef > 0:
                            expr_parts.append(f"+{coef:.4f}*{col}")
                        else:
                            expr_parts.append(f"{coef:.4f}*{col}")
                
                if lr.intercept_ > 0.01:
                    expr_parts.insert(0, f"+{lr.intercept_:.4f}")
                elif lr.intercept_ < -0.01:
                    expr_parts.insert(0, f"{lr.intercept_:.4f}")
                
                expression = "".join(expr_parts)
                if expression.startswith("+"):
                    expression = expression[1:]
                print(f"      {gp_col} â‰ˆ {expression}")
            
        except Exception as e:
            print(f"    æ‹Ÿåˆå¤±è´¥: {e}")
        
        # ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
        print(f"  ç»Ÿè®¡ä¿¡æ¯:")
        print(f"    å‡å€¼: {np.mean(gp_feature):8.4f}")
        print(f"    æ ‡å‡†å·®: {np.std(gp_feature):8.4f}")
        print(f"    æœ€å°å€¼: {np.min(gp_feature):8.4f}")
        print(f"    æœ€å¤§å€¼: {np.max(gp_feature):8.4f}")
    
    print("\n" + "="*60)
    print("ðŸ“‹ æ€»ç»“")
    print("="*60)
    print("ç”±äºŽGPLearn 0.4.1ç‰ˆæœ¬çš„é™åˆ¶ï¼Œæ— æ³•ç›´æŽ¥èŽ·å–ç‰¹å¾è¡¨è¾¾å¼ã€‚")
    print("ä½†é€šè¿‡ç›¸å…³æ€§åˆ†æžå’Œçº¿æ€§å›žå½’æ‹Ÿåˆï¼Œæˆ‘ä»¬å¯ä»¥æŽ¨æ–­å‡ºå¯èƒ½çš„è¡¨è¾¾å¼ã€‚")
    print("è¿™äº›æŽ¨æ–­çš„è¡¨è¾¾å¼å¯ä»¥å¸®åŠ©ç†è§£GPLearnç”Ÿæˆçš„ç‰¹å¾ç»„åˆæ–¹å¼ã€‚")
    
    # ä¿å­˜åˆ†æžç»“æžœ
    print("\nðŸ’¾ ä¿å­˜åˆ†æžç»“æžœ...")
    analysis_results = []
    
    for i, gp_col in enumerate(gp_features.columns):
        gp_feature = gp_features[gp_col].values
        
        # è®¡ç®—ä¸Žæ‰€æœ‰åŽŸå§‹ç‰¹å¾çš„ç›¸å…³æ€§
        correlations = []
        for j, orig_col in enumerate(X_scaled_df.columns):
            corr = np.corrcoef(X_scaled_df[orig_col], gp_feature)[0, 1]
            if not np.isnan(corr):
                correlations.append((j, orig_col, corr))
        
        correlations.sort(key=lambda x: abs(x[2]), reverse=True)
        
        # è®°å½•åˆ†æžç»“æžœ
        analysis_results.append({
            'gp_feature': gp_col,
            'top_correlation_feature': correlations[0][1] if correlations else 'N/A',
            'top_correlation_value': correlations[0][2] if correlations else 0,
            'mean': np.mean(gp_feature),
            'std': np.std(gp_feature),
            'min': np.min(gp_feature),
            'max': np.max(gp_feature)
        })
    
    analysis_df = pd.DataFrame(analysis_results)
    analysis_df.to_csv('source/ç»“æžœæ–‡ä»¶/gplearn_feature_analysis.csv', index=False)
    print("âœ… åˆ†æžç»“æžœå·²ä¿å­˜åˆ°: source/ç»“æžœæ–‡ä»¶/gplearn_feature_analysis.csv")

if __name__ == "__main__":
    analyze_gplearn_features()
