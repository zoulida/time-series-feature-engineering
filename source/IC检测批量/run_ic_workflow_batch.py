# -*- coding: utf-8 -*-
"""
ICæ£€æµ‹å·¥ä½œæµæ‰¹é‡ç‰ˆ
æ”¯æŒåˆ†æ‰¹å¤„ç†ã€è¿›åº¦ç›‘æ§ã€é”™è¯¯æ¢å¤ã€æ€§èƒ½ä¼˜åŒ–

ä¸»è¦åŠŸèƒ½ï¼š
1. æ‰¹é‡è·å–è‚¡ç¥¨æ•°æ®ï¼ˆæ”¯æŒ5000åªè‚¡ç¥¨ï¼‰
2. æ™ºèƒ½é€‰æ‹©å› å­ï¼ˆæ”¯æŒç»Ÿä¸€å› å­åº“ï¼šAlpha158ã€tsfreshç­‰ï¼‰
3. åˆ†æ‰¹ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆé¿å…å†…å­˜æº¢å‡ºï¼‰
4. é«˜æ•ˆICåˆ†æï¼ˆPearsonå’ŒSpearmanç›¸å…³ç³»æ•°ï¼‰
5. è‡ªåŠ¨æ¸…ç†æ—§æ–‡ä»¶ï¼ˆé¿å…ç£ç›˜ç©ºé—´æµªè´¹ï¼‰
6. å®æ—¶è¿›åº¦ç›‘æ§å’Œæ€§èƒ½ç»Ÿè®¡
"""

import sys
import os
import time
import logging
import psutil  # ç³»ç»Ÿèµ„æºç›‘æ§
import gc  # åƒåœ¾å›æ”¶
from datetime import datetime
from tqdm import tqdm  # è¿›åº¦æ¡æ˜¾ç¤º
import json
import pickle  # æ£€æŸ¥ç‚¹ä¿å­˜
import pandas as pd
import numpy as np

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥æ¨¡å—
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'æ•°æ®è·å–'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'å› å­åº“'))

from stock_data_fetcher import StockDataFetcher  # è‚¡ç¥¨æ•°æ®è·å–å™¨
from unified_factor_library import UnifiedFactorLibrary  # ç»Ÿä¸€å› å­åº“

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('ic_workflow_batch.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class WorkflowConfig:
    """å·¥ä½œæµé…ç½®ç±» - ç®¡ç†æ‰€æœ‰è¿è¡Œå‚æ•°å’Œæ€§èƒ½è®¾ç½®"""
    def __init__(self):
        # æµ‹è¯•é…ç½® - ç”¨äºå¿«é€ŸéªŒè¯åŠŸèƒ½
        self.test_mode = True  # æ˜¯å¦å¯ç”¨æµ‹è¯•æ¨¡å¼ï¼ˆTrue=æµ‹è¯•æ¨¡å¼ï¼ŒFalse=ç”Ÿäº§æ¨¡å¼ï¼‰
        self.test_stocks = 100  # æµ‹è¯•æ¨¡å¼è‚¡ç¥¨æ•°é‡ï¼ˆå¿«é€ŸéªŒè¯ç”¨ï¼Œå»ºè®®100-500åªï¼‰
        self.test_factors = 50  # æµ‹è¯•æ¨¡å¼å› å­æ•°é‡ï¼ˆå¿«é€ŸéªŒè¯ç”¨ï¼Œå»ºè®®20-100ä¸ªï¼‰
        
        # ç”Ÿäº§é…ç½® - ç”¨äºå¤§è§„æ¨¡æ•°æ®åˆ†æ
        self.production_stocks = 5000  # ç”Ÿäº§æ¨¡å¼è‚¡ç¥¨æ•°é‡ï¼ˆæœ€å¤§æ”¯æŒ5000åªè‚¡ç¥¨ï¼‰
        self.production_factors = 500  # ç”Ÿäº§æ¨¡å¼å› å­æ•°é‡ï¼ˆç»Ÿä¸€å› å­åº“å…¨éƒ¨å› å­ï¼‰
        
        # åˆ†æ‰¹é…ç½® - ä¼˜åŒ–å†…å­˜ä½¿ç”¨å’Œæ€§èƒ½
        self.batch_size = 500  # æ¯æ‰¹å¤„ç†çš„è‚¡ç¥¨æ•°é‡ï¼ˆå¹³è¡¡å†…å­˜ä½¿ç”¨å’Œå¤„ç†æ•ˆç‡ï¼‰
        self.max_memory_gb = 4  # æœ€å¤§å†…å­˜ä½¿ç”¨é™åˆ¶(GB)ï¼ˆè¶…è¿‡ä¼šè§¦å‘åƒåœ¾å›æ”¶ï¼‰
        
        # å¹¶è¡Œé…ç½® - æå‡è®¡ç®—æ•ˆç‡
        self.max_workers = 4  # æœ€å¤§å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆé¿å…ç³»ç»Ÿè¿‡è½½ï¼Œå»ºè®®2-8ä¸ªï¼‰
        self.enable_parallel = True  # æ˜¯å¦å¯ç”¨å¹¶è¡Œå¤„ç†ï¼ˆTrue=å¤šè¿›ç¨‹ï¼ŒFalse=å•è¿›ç¨‹ï¼‰
        
        # æ¢å¤é…ç½® - æ”¯æŒæ–­ç‚¹ç»­ä¼ 
        self.checkpoint_interval = 100  # æ£€æŸ¥ç‚¹é—´éš”ï¼ˆæ¯å¤„ç†å¤šå°‘åªè‚¡ç¥¨ä¿å­˜ä¸€æ¬¡ï¼‰
        self.enable_recovery = True  # æ˜¯å¦å¯ç”¨é”™è¯¯æ¢å¤ï¼ˆæ”¯æŒä»ä¸­æ–­ç‚¹ç»§ç»­è¿è¡Œï¼‰


class MemoryMonitor:
    """å†…å­˜ç›‘æ§ç±» - å®æ—¶ç›‘æ§ç³»ç»Ÿå†…å­˜ä½¿ç”¨æƒ…å†µï¼Œé˜²æ­¢å†…å­˜æº¢å‡º"""
    def __init__(self, max_memory_gb=4):
        self.max_memory_bytes = max_memory_gb * 1024 * 1024 * 1024  # è½¬æ¢ä¸ºå­—èŠ‚
        self.initial_memory = psutil.virtual_memory().used  # è®°å½•åˆå§‹å†…å­˜ä½¿ç”¨é‡
        
    def check_memory(self):
        """æ£€æŸ¥å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ
        
        Returns:
            dict: åŒ…å«å†…å­˜ä½¿ç”¨ä¿¡æ¯çš„å­—å…¸
                - used_gb: ç¨‹åºä½¿ç”¨çš„å†…å­˜(GB)
                - total_gb: ç³»ç»Ÿæ€»å†…å­˜ä½¿ç”¨(GB)
                - percentage: å†…å­˜ä½¿ç”¨ç™¾åˆ†æ¯”
                - is_safe: æ˜¯å¦å®‰å…¨ï¼ˆæœªè¶…è¿‡é™åˆ¶ï¼‰
        """
        current_memory = psutil.virtual_memory().used
        used_gb = (current_memory - self.initial_memory) / (1024**3)  # è®¡ç®—ç¨‹åºä½¿ç”¨çš„å†…å­˜
        total_gb = current_memory / (1024**3)  # ç³»ç»Ÿæ€»å†…å­˜ä½¿ç”¨
        
        return {
            'used_gb': round(used_gb, 2),
            'total_gb': round(total_gb, 2),
            'percentage': round((current_memory / psutil.virtual_memory().total) * 100, 1),
            'is_safe': current_memory < self.max_memory_bytes
        }
    
    def force_gc(self):
        """å¼ºåˆ¶åƒåœ¾å›æ”¶ - é‡Šæ”¾æœªä½¿ç”¨çš„å†…å­˜"""
        gc.collect()  # æ‰§è¡ŒPythonåƒåœ¾å›æ”¶
        logger.info("æ‰§è¡Œåƒåœ¾å›æ”¶")


class ProgressTracker:
    """è¿›åº¦è·Ÿè¸ªç±» - å®æ—¶æ˜¾ç¤ºå·¥ä½œæµæ‰§è¡Œè¿›åº¦ã€è€—æ—¶ç»Ÿè®¡å’Œå‰©ä½™æ—¶é—´ä¼°ç®—"""
    def __init__(self, total_steps):
        self.total_steps = total_steps  # æ€»æ­¥éª¤æ•°
        self.current_step = 0  # å½“å‰æ­¥éª¤
        self.start_time = time.time()  # å¼€å§‹æ—¶é—´
        self.checkpoints = []  # æ£€æŸ¥ç‚¹åˆ—è¡¨ï¼ˆç”¨äºé”™è¯¯æ¢å¤ï¼‰
        
    def update(self, step_name, additional_info=""):
        """æ›´æ–°è¿›åº¦ä¿¡æ¯
        
        Args:
            step_name (str): å½“å‰æ­¥éª¤åç§°
            additional_info (str): é™„åŠ ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
        """
        self.current_step += 1
        elapsed = time.time() - self.start_time  # å·²ç”¨æ—¶é—´
        remaining = (elapsed / self.current_step) * (self.total_steps - self.current_step)  # ä¼°ç®—å‰©ä½™æ—¶é—´
        
        progress_info = {
            'step': self.current_step,
            'total': self.total_steps,
            'step_name': step_name,
            'elapsed': round(elapsed, 2),
            'remaining': round(remaining, 2),
            'percentage': round((self.current_step / self.total_steps) * 100, 1),
            'additional_info': additional_info,
            'timestamp': datetime.now().isoformat()
        }
        
        self.checkpoints.append(progress_info)  # ä¿å­˜æ£€æŸ¥ç‚¹
        
        logger.info(f"è¿›åº¦: {self.current_step}/{self.total_steps} ({progress_info['percentage']}%) - {step_name}")
        if additional_info:
            logger.info(f"  è¯¦ç»†ä¿¡æ¯: {additional_info}")
            
        return progress_info
    
    def save_checkpoint(self, data, checkpoint_file):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_data = {
            'progress': {
                'current_step': self.current_step,
                'total_steps': self.total_steps,
                'checkpoints': self.checkpoints
            },
            'data': data,
            'timestamp': datetime.now().isoformat()
        }
        
        with open(checkpoint_file, 'wb') as f:
            pickle.dump(checkpoint_data, f)
        
        logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_file}")
    
    def load_checkpoint(self, checkpoint_file):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if not os.path.exists(checkpoint_file):
            return None, None
            
        with open(checkpoint_file, 'rb') as f:
            checkpoint_data = pickle.load(f)
        
        self.current_step = checkpoint_data['progress']['current_step']
        self.checkpoints = checkpoint_data['progress']['checkpoints']
        
        logger.info(f"ä»æ£€æŸ¥ç‚¹æ¢å¤: æ­¥éª¤ {self.current_step}/{self.total_steps}")
        return checkpoint_data['data'], checkpoint_data['progress']


class BatchICWorkflow:
    """æ‰¹é‡ICæ£€æµ‹å·¥ä½œæµ - æ ¸å¿ƒå·¥ä½œæµç®¡ç†ç±»
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. æ‰¹é‡è·å–è‚¡ç¥¨æ•°æ®ï¼ˆæ”¯æŒ5000åªè‚¡ç¥¨ï¼‰
    2. æ™ºèƒ½é€‰æ‹©å› å­ï¼ˆæ”¯æŒ158ä¸ªAlpha158å› å­ï¼‰
    3. åˆ†æ‰¹ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆé¿å…å†…å­˜æº¢å‡ºï¼‰
    4. é«˜æ•ˆICåˆ†æï¼ˆPearsonå’ŒSpearmanç›¸å…³ç³»æ•°ï¼‰
    5. è‡ªåŠ¨æ¸…ç†æ—§æ–‡ä»¶ï¼ˆé¿å…ç£ç›˜ç©ºé—´æµªè´¹ï¼‰
    6. å®æ—¶è¿›åº¦ç›‘æ§å’Œæ€§èƒ½ç»Ÿè®¡
    7. é”™è¯¯æ¢å¤å’Œæ–­ç‚¹ç»­ä¼ 
    """
    
    def __init__(self, config=None):
        self.config = config or WorkflowConfig()  # å·¥ä½œæµé…ç½®
        self.memory_monitor = MemoryMonitor(self.config.MAX_MEMORY_GB)  # å†…å­˜ç›‘æ§å™¨
        self.checkpoint_file = 'workflow_checkpoint.pkl'  # æ£€æŸ¥ç‚¹æ–‡ä»¶
        self.data_dir = os.path.join(os.path.dirname(__file__), 'data')  # æ•°æ®ç›®å½•
        os.makedirs(self.data_dir, exist_ok=True)  # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        
        # æ¸…ç©ºä¹‹å‰ç”Ÿæˆçš„æ•°æ®æ–‡ä»¶ï¼ˆé¿å…ç£ç›˜ç©ºé—´æµªè´¹ï¼‰
        self.cleanup_old_files()
    
    def cleanup_old_files(self):
        """æ¸…ç†ä¹‹å‰ç”Ÿæˆçš„æ•°æ®æ–‡ä»¶ - é¿å…ç£ç›˜ç©ºé—´æµªè´¹å’Œæ–‡ä»¶å†²çª
        
        æ¸…ç†çš„æ–‡ä»¶ç±»å‹ï¼š
        - è®­ç»ƒæ•°æ®æ–‡ä»¶ (training_data_batch_*.csv)
        - ICåˆ†æç»“æœ (ic_results_batch_*.json, ic_report_batch_*.csv)
        - ä¸­é—´æ–‡ä»¶ (selected_stock_codes.txt, selected_factors.*)
        - æ£€æŸ¥ç‚¹æ–‡ä»¶ (workflow_checkpoint.pkl)
        """
        try:
            # è¦æ¸…ç†çš„æ–‡ä»¶æ¨¡å¼
            file_patterns = [
                'training_data_batch_*.csv',  # è®­ç»ƒæ•°æ®æ–‡ä»¶
                'ic_results_batch_*.json',   # ICåˆ†æç»“æœJSON
                'ic_report_batch_*.csv',     # ICåˆ†ææŠ¥å‘ŠCSV
                'selected_stock_codes.txt',  # é€‰ä¸­çš„è‚¡ç¥¨ä»£ç 
                'selected_factors.csv',      # é€‰ä¸­çš„å› å­CSV
                'selected_factors.json',     # é€‰ä¸­çš„å› å­JSON
                'workflow_checkpoint.pkl'    # æ£€æŸ¥ç‚¹æ–‡ä»¶
            ]
            
            cleaned_files = []
            for pattern in file_patterns:
                import glob
                files = glob.glob(os.path.join(self.data_dir, pattern))
                for file_path in files:
                    try:
                        os.remove(file_path)
                        cleaned_files.append(os.path.basename(file_path))
                    except Exception as e:
                        logger.warning(f"æ— æ³•åˆ é™¤æ–‡ä»¶ {file_path}: {e}")
            
            if cleaned_files:
                logger.info(f"å·²æ¸…ç† {len(cleaned_files)} ä¸ªæ—§æ–‡ä»¶: {', '.join(cleaned_files[:5])}{'...' if len(cleaned_files) > 5 else ''}")
            else:
                logger.info("æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ¸…ç†çš„æ—§æ–‡ä»¶")
                
        except Exception as e:
            logger.warning(f"æ¸…ç†æ—§æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        
    def get_workflow_config(self):
        """è·å–å·¥ä½œæµé…ç½®"""
        if self.config.TEST_MODE:
            return {
                'stocks': self.config.TEST_STOCKS,
                'factors': self.config.TEST_FACTORS,
                'mode': 'æµ‹è¯•æ¨¡å¼'
            }
        else:
            return {
                'stocks': self.config.PRODUCTION_STOCKS,
                'factors': self.config.PRODUCTION_FACTORS,
                'mode': 'ç”Ÿäº§æ¨¡å¼'
            }
    
    def check_dependencies(self):
        """æ£€æŸ¥ä¾èµ–é¡¹"""
        logger.info("æ£€æŸ¥ä¾èµ–é¡¹...")
        
        # æ£€æŸ¥å†…å­˜
        memory_info = self.memory_monitor.check_memory()
        logger.info(f"å†…å­˜ä½¿ç”¨: {memory_info['used_gb']}GB / {memory_info['total_gb']}GB ({memory_info['percentage']}%)")
        
        if not memory_info['is_safe']:
            logger.warning("å†…å­˜ä½¿ç”¨æ¥è¿‘é™åˆ¶ï¼Œå»ºè®®å‡å°‘æ‰¹æ¬¡å¤§å°")
        
        # æ£€æŸ¥ç£ç›˜ç©ºé—´
        disk_usage = psutil.disk_usage('.')
        free_gb = disk_usage.free / (1024**3)
        logger.info(f"å¯ç”¨ç£ç›˜ç©ºé—´: {free_gb:.1f}GB")
        
        if free_gb < 2:
            logger.warning("ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œå¯èƒ½å½±å“æ–‡ä»¶ä¿å­˜")
        
        logger.info("ä¾èµ–é¡¹æ£€æŸ¥å®Œæˆ")
        return True
    
    def run_batch_workflow(self):
        """è¿è¡Œæ‰¹é‡å·¥ä½œæµ"""
        try:
            logger.info("=" * 60)
            logger.info("ICæ£€æµ‹å·¥ä½œæµæ‰¹é‡ç‰ˆå¼€å§‹æ‰§è¡Œ")
            logger.info("=" * 60)
            
            # è·å–é…ç½®
            workflow_config = self.get_workflow_config()
            logger.info(f"è¿è¡Œæ¨¡å¼: {workflow_config['mode']}")
            logger.info(f"è‚¡ç¥¨æ•°é‡: {workflow_config['stocks']}")
            logger.info(f"å› å­æ•°é‡: {workflow_config['factors']}")
            
            # æ£€æŸ¥ä¾èµ–
            if not self.check_dependencies():
                return False
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ£€æŸ¥ç‚¹å¯æ¢å¤
            if self.config.ENABLE_RECOVERY:
                checkpoint_data, progress = self.load_checkpoint()
                if checkpoint_data:
                    logger.info("ä»æ£€æŸ¥ç‚¹æ¢å¤å·¥ä½œæµ")
                    return self.resume_from_checkpoint(checkpoint_data, progress)
            
            # å¼€å§‹æ–°çš„å·¥ä½œæµ
            return self.start_new_workflow(workflow_config)
            
        except Exception as e:
            logger.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}")
            return False
    
    def load_checkpoint(self):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if not os.path.exists(self.checkpoint_file):
            return None, None
            
        try:
            with open(self.checkpoint_file, 'rb') as f:
                checkpoint_data = pickle.load(f)
            return checkpoint_data, checkpoint_data.get('progress')
        except Exception as e:
            logger.warning(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {str(e)}")
            return None, None
    
    def resume_from_checkpoint(self, checkpoint_data, progress):
        """ä»æ£€æŸ¥ç‚¹æ¢å¤"""
        # è¿™é‡Œå¯ä»¥å®ç°ä»æ£€æŸ¥ç‚¹æ¢å¤çš„é€»è¾‘
        logger.info("æ£€æŸ¥ç‚¹æ¢å¤åŠŸèƒ½å¾…å®ç°")
        return False
    
    def start_new_workflow(self, workflow_config):
        """å¼€å§‹æ–°çš„å·¥ä½œæµ - æ‰§è¡Œå®Œæ•´çš„ICæ£€æµ‹æµç¨‹
        
        å·¥ä½œæµåŒ…å«4ä¸ªä¸»è¦æ­¥éª¤ï¼š
        1. è·å–è‚¡ç¥¨æ•°æ® - ä»æ•°æ®æºåŠ è½½æŒ‡å®šæ•°é‡çš„è‚¡ç¥¨æ•°æ®
        2. é€‰æ‹©å› å­ - ä»Alpha158å› å­åº“ä¸­é€‰æ‹©æŒ‡å®šæ•°é‡çš„å› å­
        3. ç”Ÿæˆè®­ç»ƒæ•°æ® - è®¡ç®—å› å­å€¼å’Œæ”¶ç›Šç‡ï¼Œåˆ†æ‰¹å¤„ç†é¿å…å†…å­˜æº¢å‡º
        4. ICåˆ†æ - è®¡ç®—æ¯ä¸ªå› å­çš„ä¿¡æ¯ç³»æ•°ï¼Œè¯„ä¼°å› å­æœ‰æ•ˆæ€§
        
        Args:
            workflow_config (dict): å·¥ä½œæµé…ç½®å‚æ•°
            
        Returns:
            bool: å·¥ä½œæµæ˜¯å¦æ‰§è¡ŒæˆåŠŸ
        """
        # åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨
        total_steps = 4  # 4ä¸ªä¸»è¦æ­¥éª¤
        progress_tracker = ProgressTracker(total_steps)
        
        try:
            total_start_time = time.time()
            
            # æ­¥éª¤1: è·å–è‚¡ç¥¨æ•°æ®
            step1_start = time.time()
            progress_tracker.update("è·å–è‚¡ç¥¨æ•°æ®", f"ç›®æ ‡: {workflow_config['stocks']}åªè‚¡ç¥¨")
            stock_data = self.get_stock_data_batch(workflow_config['stocks'])
            step1_time = time.time() - step1_start
            logger.info(f"æ­¥éª¤1å®Œæˆï¼Œè€—æ—¶: {step1_time:.2f}ç§’")
            
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨æƒ…å†µ
            memory_info = self.memory_monitor.check_memory()
            if not memory_info['is_safe']:
                self.memory_monitor.force_gc()  # å†…å­˜ä¸è¶³æ—¶å¼ºåˆ¶åƒåœ¾å›æ”¶
            
            # æ­¥éª¤2: é€‰æ‹©å› å­
            step2_start = time.time()
            progress_tracker.update("é€‰æ‹©å› å­", f"ç›®æ ‡: {workflow_config['factors']}ä¸ªå› å­")
            selected_factors = self.select_factors_batch(workflow_config['factors'])
            step2_time = time.time() - step2_start
            logger.info(f"æ­¥éª¤2å®Œæˆï¼Œè€—æ—¶: {step2_time:.2f}ç§’")
            
            # æ­¥éª¤3: ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰
            step3_start = time.time()
            progress_tracker.update("ç”Ÿæˆè®­ç»ƒæ•°æ®", "åˆ†æ‰¹å¤„ç†ä¸­...")
            training_data = self.generate_training_data_batch(stock_data, selected_factors, progress_tracker)
            step3_time = time.time() - step3_start
            logger.info(f"æ­¥éª¤3å®Œæˆï¼Œè€—æ—¶: {step3_time:.2f}ç§’")
            
            # æ­¥éª¤4: ICåˆ†æ
            step4_start = time.time()
            progress_tracker.update("ICåˆ†æ", f"åˆ†æ{len(selected_factors)}ä¸ªå› å­")
            ic_results = self.perform_ic_analysis_batch(training_data, selected_factors)
            step4_time = time.time() - step4_start
            logger.info(f"æ­¥éª¤4å®Œæˆï¼Œè€—æ—¶: {step4_time:.2f}ç§’")
            
            # ä¿å­˜æœ€ç»ˆç»“æœ
            self.save_final_results(ic_results, progress_tracker)
            
            total_time = time.time() - total_start_time
            
            # è¾“å‡ºè¯¦ç»†çš„æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯
            logger.info("=" * 60)
            logger.info("å·¥ä½œæµæ‰§è¡Œå®Œæˆ")
            logger.info("=" * 60)
            logger.info("å„é˜¶æ®µè€—æ—¶ç»Ÿè®¡:")
            logger.info(f"  æ­¥éª¤1 - è·å–è‚¡ç¥¨æ•°æ®: {step1_time:.2f}ç§’ ({step1_time/total_time*100:.1f}%)")
            logger.info(f"  æ­¥éª¤2 - é€‰æ‹©å› å­: {step2_time:.2f}ç§’ ({step2_time/total_time*100:.1f}%)")
            logger.info(f"  æ­¥éª¤3 - ç”Ÿæˆè®­ç»ƒæ•°æ®: {step3_time:.2f}ç§’ ({step3_time/total_time*100:.1f}%)")
            logger.info(f"  æ­¥éª¤4 - ICåˆ†æ: {step4_time:.2f}ç§’ ({step4_time/total_time*100:.1f}%)")
            logger.info(f"  æ€»è€—æ—¶: {total_time:.2f}ç§’")
            logger.info("=" * 60)
            
            return True
            
        except Exception as e:
            logger.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}")
            # ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆæ”¯æŒé”™è¯¯æ¢å¤ï¼‰
            if self.config.ENABLE_RECOVERY:
                progress_tracker.save_checkpoint({'error': str(e)}, self.checkpoint_file)
            return False
    
    def get_stock_data_batch(self, num_stocks):
        """æ‰¹é‡è·å–è‚¡ç¥¨æ•°æ®"""
        try:
            # åˆ›å»ºæ•°æ®è·å–å™¨
            fetcher = StockDataFetcher()
            
            # è·å–æ‰€æœ‰å¯ç”¨è‚¡ç¥¨
            all_stocks = fetcher.get_available_stocks()
            logger.info(f"å‘ç° {len(all_stocks)} åªè‚¡ç¥¨")
            
            if len(all_stocks) < num_stocks:
                logger.warning(f"å¯ç”¨è‚¡ç¥¨æ•°é‡ä¸è¶³ï¼Œéœ€è¦{num_stocks}åªï¼Œå®é™…åªæœ‰{len(all_stocks)}åª")
                num_stocks = len(all_stocks)
            
            # é€‰æ‹©æŒ‡å®šæ•°é‡çš„è‚¡ç¥¨
            selected_stocks = all_stocks[:num_stocks]
            logger.info(f"é€‰æ‹©çš„è‚¡ç¥¨: {len(selected_stocks)}åª")
            
            # è·å–è‚¡ç¥¨æ•°æ®
            stock_data = {}
            max_start_date = None
            min_end_date = None
            
            logger.info("æ­£åœ¨åˆ†æè‚¡ç¥¨æ•°æ®æ—¶é—´èŒƒå›´...")
            for i, stock_code in enumerate(tqdm(selected_stocks, desc="åŠ è½½è‚¡ç¥¨æ•°æ®")):
                try:
                    df = fetcher.load_single_stock_data(stock_code)
                    stock_data[stock_code] = df
                    
                    # æ›´æ–°æœ€å¤§æ—¶é—´èŒƒå›´
                    stock_start = df['date'].min()
                    stock_end = df['date'].max()
                    
                    if max_start_date is None or stock_start > max_start_date:
                        max_start_date = stock_start
                    if min_end_date is None or stock_end < min_end_date:
                        min_end_date = stock_end
                        
                except Exception as e:
                    logger.error(f"è·å–è‚¡ç¥¨ {stock_code} æ•°æ®å¤±è´¥: {str(e)}")
                    continue
            
            if not stock_data:
                raise ValueError("æœªèƒ½è·å–ä»»ä½•è‚¡ç¥¨æ•°æ®")
            
            # åªè¿‡æ»¤æ‰0æ¡è®°å½•çš„è‚¡ç¥¨ï¼Œå…¶ä»–éƒ½ä¿ç•™
            logger.info("è¿‡æ»¤0æ¡è®°å½•çš„è‚¡ç¥¨...")
            valid_stock_data = {}
            empty_stocks = []
            
            for stock_code, df in stock_data.items():
                if len(df) > 0:  # åªè¦æœ‰æ•°æ®å°±ä¿ç•™
                    valid_stock_data[stock_code] = df
                else:
                    empty_stocks.append(stock_code)
            
            logger.info(f"æœ‰æ•ˆæ•°æ®è‚¡ç¥¨: {len(valid_stock_data)} åª")
            logger.info(f"ç©ºæ•°æ®è‚¡ç¥¨: {len(empty_stocks)} åª")
            if empty_stocks:
                logger.info("ç©ºæ•°æ®è‚¡ç¥¨:")
                for stock_code in empty_stocks[:5]:
                    logger.info(f"  {stock_code}")
            
            # ç›´æ¥ä½¿ç”¨æ‰€æœ‰æœ‰æ•ˆæ•°æ®ï¼Œä¸è¿›è¡Œæ—¶é—´èŒƒå›´è¿‡æ»¤
            if valid_stock_data:
                filtered_stock_data = valid_stock_data
                logger.info(f"ä¿ç•™æ‰€æœ‰æœ‰æ•ˆæ•°æ®ï¼Œå…± {len(filtered_stock_data)} åªè‚¡ç¥¨")
                
                # ç»Ÿè®¡æ•°æ®é‡åˆ†å¸ƒ
                data_counts = [len(df) for df in filtered_stock_data.values()]
                logger.info(f"æ•°æ®é‡ç»Ÿè®¡: æœ€å°‘ {min(data_counts)} æ¡ï¼Œæœ€å¤š {max(data_counts)} æ¡ï¼Œå¹³å‡ {sum(data_counts)/len(data_counts):.1f} æ¡")
            else:
                raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæ•°æ®çš„è‚¡ç¥¨")
            
            # ä¿å­˜è‚¡ç¥¨ä»£ç åˆ—è¡¨
            stock_codes_path = os.path.join(self.data_dir, 'selected_stock_codes.txt')
            with open(stock_codes_path, 'w', encoding='utf-8') as f:
                for stock_code in filtered_stock_data.keys():
                    f.write(f"{stock_code}\n")
            logger.info(f"è‚¡ç¥¨ä»£ç åˆ—è¡¨å·²ä¿å­˜åˆ°: {stock_codes_path}")
            
            return filtered_stock_data
            
        except Exception as e:
            logger.error(f"è·å–è‚¡ç¥¨æ•°æ®å¤±è´¥: {str(e)}")
            raise
    
    def select_factors_batch(self, num_factors):
        """æ‰¹é‡é€‰æ‹©å› å­"""
        try:
            # åˆ›å»ºç»Ÿä¸€å› å­åº“å®ä¾‹
            factor_lib = UnifiedFactorLibrary()
            
            # è·å–æ‰€æœ‰å› å­
            all_factors = factor_lib.list_factors()
            logger.info(f"ç»Ÿä¸€å› å­åº“æ€»å…±æœ‰ {len(all_factors)} ä¸ªå› å­")
            
            if len(all_factors) < num_factors:
                logger.warning(f"å¯ç”¨å› å­æ•°é‡ä¸è¶³ï¼Œéœ€è¦{num_factors}ä¸ªï¼Œå®é™…åªæœ‰{len(all_factors)}ä¸ª")
                num_factors = len(all_factors)
            
            # æŒ‰ç±»åˆ«åˆ†ç»„å› å­
            factor_categories = {}
            for factor_name in all_factors:
                factor_info = factor_lib.get_factor_info(factor_name)
                category = factor_info.get('category', 'æœªçŸ¥')
                if category not in factor_categories:
                    factor_categories[category] = []
                factor_categories[category].append(factor_name)
            
            logger.info("å› å­ç±»åˆ«åˆ†å¸ƒ:")
            for category, factors in factor_categories.items():
                logger.info(f"  {category}: {len(factors)} ä¸ªå› å­")
            
            # ä»æ¯ä¸ªç±»åˆ«ä¸­éšæœºé€‰æ‹©å› å­ï¼Œç¡®ä¿å¤šæ ·æ€§
            selected_factors = []
            factors_per_category = max(1, num_factors // len(factor_categories))
            remaining_factors = num_factors
            
            for category, factors in factor_categories.items():
                if remaining_factors <= 0:
                    break
                
                # ä»å½“å‰ç±»åˆ«é€‰æ‹©å› å­
                select_count = min(factors_per_category, remaining_factors, len(factors))
                selected_from_category = np.random.choice(factors, select_count, replace=False).tolist()
                selected_factors.extend(selected_from_category)
                remaining_factors -= select_count
                
                logger.info(f"ä» {category} ç±»åˆ«é€‰æ‹©äº† {select_count} ä¸ªå› å­")
            
            # å¦‚æœè¿˜æœ‰å‰©ä½™å› å­ï¼Œéšæœºè¡¥å……
            if remaining_factors > 0:
                remaining_all = [f for f in all_factors if f not in selected_factors]
                if remaining_all:
                    additional_count = min(remaining_factors, len(remaining_all))
                    additional_factors = np.random.choice(remaining_all, additional_count, replace=False).tolist()
                    selected_factors.extend(additional_factors)
                    logger.info(f"éšæœºè¡¥å……äº† {additional_count} ä¸ªå› å­")
            
            logger.info(f"æœ€ç»ˆé€‰æ‹©äº† {len(selected_factors)} ä¸ªå› å­")
            
            # ä¿å­˜é€‰ä¸­çš„å› å­ä¿¡æ¯
            self.save_selected_factors(selected_factors, factor_lib)
            
            return selected_factors
            
        except Exception as e:
            logger.error(f"é€‰æ‹©å› å­å¤±è´¥: {str(e)}")
            raise
    
    def save_selected_factors(self, selected_factors, factor_lib):
        """ä¿å­˜é€‰ä¸­çš„å› å­ä¿¡æ¯"""
        try:
            # åˆ›å»ºå› å­è¯¦æƒ…
            factor_details = []
            for factor_name in selected_factors:
                factor_info = factor_lib.get_factor_info(factor_name)
                if factor_info:
                    factor_details.append({
                        'factor_name': factor_name,
                        'function_name': factor_info.get('function_name', ''),
                        'category': factor_info.get('category', ''),
                        'description': factor_info.get('description', ''),
                        'expression': factor_info.get('expression', ''),
                        'formula': factor_info.get('formula', '')
                    })
            
            # ä¿å­˜ä¸ºCSV
            factor_df = pd.DataFrame(factor_details)
            csv_path = os.path.join(self.data_dir, 'selected_factors.csv')
            factor_df.to_csv(csv_path, index=False, encoding='utf-8-sig')
            logger.info(f"é€‰ä¸­å› å­è¯¦æƒ…å·²ä¿å­˜åˆ°: {csv_path}")
            
            # ä¿å­˜ä¸ºJSON
            json_path = os.path.join(self.data_dir, 'selected_factors.json')
            with open(json_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'selected_factors': selected_factors,
                    'factor_details': factor_details,
                    'selection_time': datetime.now().isoformat(),
                    'total_count': len(selected_factors)
                }, f, ensure_ascii=False, indent=2)
            logger.info(f"é€‰ä¸­å› å­ä¿¡æ¯å·²ä¿å­˜åˆ°: {json_path}")
            
        except Exception as e:
            logger.error(f"ä¿å­˜å› å­ä¿¡æ¯å¤±è´¥: {str(e)}")
    
    def generate_training_data_batch(self, stock_data, selected_factors, progress_tracker):
        """æ‰¹é‡ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆåˆ†æ‰¹å¤„ç†ï¼‰"""
        try:
            logger.info("å¼€å§‹åˆ†æ‰¹å¤„ç†è®­ç»ƒæ•°æ®ç”Ÿæˆ...")
            
            # åˆ†æ‰¹å¤„ç†è‚¡ç¥¨
            stock_codes = list(stock_data.keys())
            batches = [stock_codes[i:i+self.config.BATCH_SIZE] 
                      for i in range(0, len(stock_codes), self.config.BATCH_SIZE)]
            
            logger.info(f"åˆ†ä¸º {len(batches)} ä¸ªæ‰¹æ¬¡å¤„ç†ï¼Œæ¯æ‰¹æœ€å¤š {self.config.BATCH_SIZE} åªè‚¡ç¥¨")
            
            all_training_data = []
            
            for batch_idx, batch in enumerate(batches):
                logger.info(f"å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{len(batches)}: {len(batch)}åªè‚¡ç¥¨")
                
                # å¤„ç†å½“å‰æ‰¹æ¬¡
                batch_data = self.process_stock_batch(batch, stock_data, selected_factors)
                all_training_data.append(batch_data)
                
                # æ£€æŸ¥å†…å­˜
                memory_info = self.memory_monitor.check_memory()
                logger.info(f"å†…å­˜ä½¿ç”¨: {memory_info['used_gb']}GB ({memory_info['percentage']}%)")
                
                if not memory_info['is_safe']:
                    self.memory_monitor.force_gc()
                
                # ä¿å­˜ä¸­é—´æ£€æŸ¥ç‚¹
                if self.config.ENABLE_RECOVERY and (batch_idx + 1) % self.config.CHECKPOINT_INTERVAL == 0:
                    progress_tracker.save_checkpoint({
                        'processed_batches': batch_idx + 1,
                        'total_batches': len(batches),
                        'training_data': all_training_data
                    }, self.checkpoint_file)
            
            # åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡æ•°æ®
            logger.info("åˆå¹¶æ‰€æœ‰æ‰¹æ¬¡æ•°æ®...")
            combined_data = pd.concat(all_training_data, ignore_index=True)
            
            # ä¿å­˜è®­ç»ƒæ•°æ®
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            training_data_path = os.path.join(self.data_dir, f'training_data_batch_{timestamp}.csv')
            combined_data.to_csv(training_data_path, index=False, encoding='utf-8-sig')
            logger.info(f"è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {training_data_path}")
            
            return combined_data
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆè®­ç»ƒæ•°æ®å¤±è´¥: {str(e)}")
            raise
    
    def process_stock_batch(self, stock_batch, stock_data, selected_factors):
        """å¤„ç†å•ä¸ªè‚¡ç¥¨æ‰¹æ¬¡"""
        try:
            # åˆ›å»ºç»Ÿä¸€å› å­åº“å®ä¾‹
            factor_lib = UnifiedFactorLibrary()
            
            all_training_data = []
            
            for stock_code in stock_batch:
                if stock_code not in stock_data:
                    continue
                
                df = stock_data[stock_code].copy()
                logger.info(f"å¤„ç†è‚¡ç¥¨ {stock_code}: {len(df)} æ¡è®°å½•")
                
                # è®¡ç®—å› å­ - ä½¿ç”¨æ‰¹é‡æ·»åŠ é¿å…ç¢ç‰‡åŒ–
                factor_data = {}
                for factor_name in selected_factors:
                    factor_col = f'factor_{factor_name}'
                    try:
                        factor_values = self.calculate_factor(df, factor_name, factor_lib)
                        factor_data[factor_col] = factor_values
                    except Exception as e:
                        logger.warning(f"è®¡ç®—å› å­ {factor_name} å¤±è´¥: {str(e)}")
                        factor_data[factor_col] = np.full(len(df), np.nan)
                
                # è®¡ç®—æ”¶ç›Šç‡
                returns = self.calculate_returns(df)
                factor_data.update(returns)
                
                # æ‰¹é‡æ·»åŠ æ‰€æœ‰åˆ—
                if factor_data:
                    factor_df = pd.DataFrame(factor_data, index=df.index)
                    df = pd.concat([df, factor_df], axis=1)
                
                # æ·»åŠ æŠ€æœ¯æŒ‡æ ‡
                df = self.add_technical_indicators(df)
                
                # æ·»åŠ è‚¡ç¥¨ä»£ç 
                df['stock_code'] = stock_code
                
                all_training_data.append(df)
            
            # åˆå¹¶å½“å‰æ‰¹æ¬¡æ•°æ®
            if all_training_data:
                batch_df = pd.concat(all_training_data, ignore_index=True)
                # ç§»é™¤åŒ…å«NaNçš„è¡Œ
                initial_rows = len(batch_df)
                batch_df = batch_df.dropna()
                removed_rows = initial_rows - len(batch_df)
                if removed_rows > 0:
                    logger.info(f"æ‰¹æ¬¡ç§»é™¤äº† {removed_rows} è¡ŒåŒ…å«NaNçš„æ•°æ®")
                
                return batch_df
            else:
                return pd.DataFrame()
                
        except Exception as e:
            logger.error(f"å¤„ç†è‚¡ç¥¨æ‰¹æ¬¡å¤±è´¥: {str(e)}")
            raise
    
    def calculate_factor(self, data, factor_name, factor_lib):
        """è®¡ç®—å•ä¸ªå› å­"""
        try:
            factor_info = factor_lib.get_factor_info(factor_name)
            if not factor_info:
                return np.nan
            
            # æ ¹æ®å› å­æ¥æºé€‰æ‹©ä¸åŒçš„è®¡ç®—æ–¹å¼
            source = factor_info.get('source', '')
            
            if source == 'Alpha158':
                # Alpha158å› å­ä½¿ç”¨åŸå§‹è®¡ç®—é€»è¾‘
                return self._calculate_alpha158_factor(data, factor_name, factor_info)
            elif source == 'tsfresh':
                # tsfreshå› å­ä½¿ç”¨æ—¶é—´åºåˆ—ç‰¹å¾æå–
                return self._calculate_tsfresh_factor(data, factor_name, factor_info)
            else:
                # å…¶ä»–å› å­ä½¿ç”¨ç®€åŒ–è®¡ç®—
                return self._calculate_generic_factor(data, factor_name, factor_info)
            
        except Exception as e:
            logger.warning(f"è®¡ç®—å› å­ {factor_name} å¤±è´¥: {str(e)}")
            return np.nan
    
    def _calculate_alpha158_factor(self, data, factor_name, factor_info):
        """è®¡ç®—Alpha158å› å­"""
        # è¿™é‡Œåº”è¯¥å®ç°Alpha158å› å­çš„å…·ä½“è®¡ç®—é€»è¾‘
        # ç›®å‰ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
        return np.random.randn(len(data))
    
    def _calculate_tsfresh_factor(self, data, factor_name, factor_info):
        """è®¡ç®—tsfreshå› å­"""
        # è¿™é‡Œåº”è¯¥å®ç°tsfreshå› å­çš„å…·ä½“è®¡ç®—é€»è¾‘
        # ç›®å‰ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
        return np.random.randn(len(data))
    
    def _calculate_generic_factor(self, data, factor_name, factor_info):
        """è®¡ç®—é€šç”¨å› å­"""
        # æ ¹æ®å› å­è¡¨è¾¾å¼æˆ–å‡½æ•°åè®¡ç®—
        expression = factor_info.get('expression', '')
        if expression:
            # è¿™é‡Œåº”è¯¥è§£æè¡¨è¾¾å¼å¹¶è®¡ç®—
            # ç›®å‰ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
            return np.random.randn(len(data))
        else:
            # ä½¿ç”¨éšæœºå€¼ä½œä¸ºå ä½ç¬¦
            return np.random.randn(len(data))
    
    def calculate_returns(self, data):
        """è®¡ç®—æ”¶ç›Šç‡"""
        returns = {}
        
        # è®¡ç®—æœªæ¥15å¤©ä¸­æ”¶ç›Šç‡çš„æœ€å¤§å€¼
        if len(data) > 15:
            # è®¡ç®—æœªæ¥1-15å¤©çš„æ‰€æœ‰æ”¶ç›Šç‡
            future_returns = []
            for i in range(1, 16):  # 1å¤©åˆ°15å¤©
                future_return = data['close'].shift(-i) / data['close'] - 1
                future_returns.append(future_return)
            
            # å°†æœªæ¥æ”¶ç›Šç‡ç»„åˆæˆDataFrame
            future_returns_df = pd.DataFrame(future_returns).T
            # è®¡ç®—æ¯è¡Œï¼ˆæ¯ä¸ªæ—¶é—´ç‚¹ï¼‰çš„æœ€å¤§æ”¶ç›Šç‡
            max_future_return = future_returns_df.max(axis=1)
            returns['return_15d'] = max_future_return
            
            # ä¿ç•™è¿‡å»15å¤©æ”¶ç›Šç‡ä½œä¸ºå‚è€ƒ
            past_return = data['close'] / data['close'].shift(15) - 1
            returns['past_return_15d'] = past_return
        
        return returns
    
    def add_technical_indicators(self, data):
        """æ·»åŠ æŠ€æœ¯æŒ‡æ ‡ - ä½¿ç”¨æ‰¹é‡æ·»åŠ é¿å…ç¢ç‰‡åŒ–"""
        # å‡†å¤‡æ‰€æœ‰æŠ€æœ¯æŒ‡æ ‡æ•°æ®
        indicators = {}
        
        # ä»·æ ¼å˜åŒ–
        indicators['price_change'] = (data['close'] - data['close'].shift(1)) / data['close'].shift(1)
        indicators['volume_change'] = (data['volume'] - data['volume'].shift(1)) / (data['volume'].shift(1) + 1e-12)
        indicators['high_low_ratio'] = data['high'] / data['low']
        indicators['close_open_ratio'] = data['close'] / data['open']
        
        # ç§»åŠ¨å¹³å‡
        if len(data) > 20:
            indicators['sma_5'] = data['close'].rolling(5).mean()
            indicators['sma_20'] = data['close'].rolling(20).mean()
            # å¸ƒæ—å¸¦
            bb_mean = data['close'].rolling(20).mean()
            bb_std = data['close'].rolling(20).std()
            indicators['bb_upper'] = bb_mean + 2 * bb_std
            indicators['bb_lower'] = bb_mean - 2 * bb_std
            indicators['bb_width'] = (indicators['bb_upper'] - indicators['bb_lower']) / data['close']
        
        # æ‰¹é‡æ·»åŠ æ‰€æœ‰æŠ€æœ¯æŒ‡æ ‡
        if indicators:
            indicators_df = pd.DataFrame(indicators, index=data.index)
            data = pd.concat([data, indicators_df], axis=1)
        
        return data
    
    def perform_ic_analysis_batch(self, training_data, selected_factors):
        """æ‰¹é‡ICåˆ†æ - è®¡ç®—æ‰€æœ‰å› å­çš„ä¿¡æ¯ç³»æ•°(Information Coefficient)
        
        ICåˆ†ææ˜¯é‡åŒ–æŠ•èµ„ä¸­çš„æ ¸å¿ƒæŒ‡æ ‡ï¼Œç”¨äºè¡¡é‡å› å­ä¸æœªæ¥æ”¶ç›Šç‡çš„ç›¸å…³æ€§ï¼š
        - Pearson IC: çº¿æ€§ç›¸å…³ç³»æ•°ï¼Œè¡¡é‡çº¿æ€§å…³ç³»å¼ºåº¦
        - Spearman IC: ç§©ç›¸å…³ç³»æ•°ï¼Œè¡¡é‡å•è°ƒå…³ç³»å¼ºåº¦
        
        Args:
            training_data (DataFrame): åŒ…å«å› å­å€¼å’Œæ”¶ç›Šç‡çš„è®­ç»ƒæ•°æ®
            selected_factors (list): é€‰ä¸­çš„å› å­åˆ—è¡¨
            
        Returns:
            dict: ICåˆ†æç»“æœï¼ŒåŒ…å«æ¯ä¸ªå› å­çš„ICå€¼å’Œç»Ÿè®¡ä¿¡æ¯
        """
        try:
            logger.info("å¼€å§‹ICåˆ†æ...")
            
            # è·å–å› å­åˆ—å’Œæ”¶ç›Šç‡åˆ—
            factor_cols = [col for col in training_data.columns if col.startswith('factor_')]
            return_cols = [col for col in training_data.columns if col.startswith('return_15d')]
            
            logger.info(f"æ‰¾åˆ° {len(factor_cols)} ä¸ªå› å­åˆ—")
            logger.info(f"æ‰¾åˆ° {len(return_cols)} ä¸ªæ”¶ç›Šç‡åˆ—")
            
            if not factor_cols or not return_cols:
                raise ValueError("æœªæ‰¾åˆ°å› å­åˆ—æˆ–æ”¶ç›Šç‡åˆ—")
            
            # è¿›è¡ŒICåˆ†æ
            ic_results = {}
            
            for factor_col in tqdm(factor_cols, desc="ICåˆ†æ"):
                factor_name = factor_col.replace('factor_', '')
                
                for return_col in return_cols:
                    return_period = return_col.replace('return_', '').replace('d', '')
                    
                    # è®¡ç®—IC - åªä½¿ç”¨æœ‰æ•ˆæ•°æ®ï¼ˆéNaNå€¼ï¼‰
                    valid_data = training_data[[factor_col, return_col]].dropna()
                    if len(valid_data) > 10:  # è‡³å°‘éœ€è¦10ä¸ªæœ‰æ•ˆæ ·æœ¬
                        # çš®å°”é€Šç›¸å…³ç³»æ•° - è¡¡é‡çº¿æ€§å…³ç³»
                        ic_value = valid_data[factor_col].corr(valid_data[return_col])
                        # æ–¯çš®å°”æ›¼ç›¸å…³ç³»æ•° - è¡¡é‡å•è°ƒå…³ç³»
                        spearman_ic = valid_data[factor_col].corr(valid_data[return_col], method='spearman')
                        
                        ic_results[f"{factor_name}_{return_period}d"] = {
                            'pearson_ic': ic_value,
                            'spearman_ic': spearman_ic,
                            'sample_size': len(valid_data)
                        }
                        
                        logger.info(f"  {factor_name} - {return_period}å¤©æ”¶ç›Šç‡IC: {ic_value:.4f} (Spearman: {spearman_ic:.4f})")
            
            return ic_results
            
        except Exception as e:
            logger.error(f"ICåˆ†æå¤±è´¥: {str(e)}")
            raise
    
    def save_final_results(self, ic_results, progress_tracker):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        try:
            # ä¿å­˜ICç»“æœ
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            results_file = os.path.join(self.data_dir, f'ic_results_batch_{timestamp}.json')
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(ic_results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ICç»“æœå·²ä¿å­˜åˆ°: {results_file}")
            
            # ç”ŸæˆICæŠ¥å‘Š
            self.generate_ic_report(ic_results, timestamp)
            
            # æ¸…ç†æ£€æŸ¥ç‚¹æ–‡ä»¶
            if os.path.exists(self.checkpoint_file):
                os.remove(self.checkpoint_file)
                logger.info("æ£€æŸ¥ç‚¹æ–‡ä»¶å·²æ¸…ç†")
                
        except Exception as e:
            logger.error(f"ä¿å­˜æœ€ç»ˆç»“æœå¤±è´¥: {str(e)}")
    
    def generate_ic_report(self, ic_results, timestamp):
        """ç”ŸæˆICæŠ¥å‘Š"""
        try:
            # åˆ›å»ºICæŠ¥å‘ŠDataFrame
            report_data = []
            for factor_return, metrics in ic_results.items():
                factor_name, return_period = factor_return.rsplit('_', 1)
                report_data.append({
                    'factor_name': factor_name,
                    'return_period': return_period,
                    'pearson_ic': metrics['pearson_ic'],
                    'spearman_ic': metrics['spearman_ic'],
                    'sample_size': metrics['sample_size']
                })
            
            report_df = pd.DataFrame(report_data)
            
            # ä¿å­˜æŠ¥å‘Š
            report_file = os.path.join(self.data_dir, f'ic_report_batch_{timestamp}.csv')
            report_df.to_csv(report_file, index=False, encoding='utf-8-sig')
            logger.info(f"ICæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
            
            # æ˜¾ç¤ºæ‘˜è¦
            print("\n=== ICåˆ†æç»“æœæ‘˜è¦ ===")
            print(f"æ€»å› å­æ•°: {len(report_df['factor_name'].unique())}")
            print(f"å¹³å‡Pearson IC: {report_df['pearson_ic'].mean():.4f}")
            print(f"å¹³å‡Spearman IC: {report_df['spearman_ic'].mean():.4f}")
            
            # æ˜¾ç¤ºå‰10ä¸ªæœ€ä½³å› å­ï¼ˆåŒ…å«æè¿°ä¿¡æ¯ï¼‰
            best_factors = report_df.nlargest(10, 'pearson_ic')
            print("\n=== å‰10ä¸ªæœ€ä½³å› å­ ===")
            
            # è·å–å› å­æè¿°ä¿¡æ¯
            try:
                from unified_factor_library import UnifiedFactorLibrary
                factor_lib = UnifiedFactorLibrary()
                
                for i, (_, row) in enumerate(best_factors.iterrows(), 1):
                    factor_name = row['factor_name']
                    ic_value = row['pearson_ic']
                    
                    # è·å–å› å­æè¿°
                    factor_info = factor_lib.get_factor_info(factor_name)
                    description = factor_info.get('description', 'æš‚æ— æè¿°') if factor_info else 'æš‚æ— æè¿°'
                    category = factor_info.get('category', 'æœªçŸ¥ç±»åˆ«') if factor_info else 'æœªçŸ¥ç±»åˆ«'
                    source = factor_info.get('source', 'æœªçŸ¥æ¥æº') if factor_info else 'æœªçŸ¥æ¥æº'
                    
                    print(f"{i:2d}. {factor_name}: {ic_value:.4f}")
                    print(f"    æ¥æº: {source}")
                    print(f"    ç±»åˆ«: {category}")
                    print(f"    æè¿°: {description}")
                    print()
                    
            except Exception as e:
                # å¦‚æœè·å–æè¿°å¤±è´¥ï¼Œåªæ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯
                logger.warning(f"è·å–å› å­æè¿°å¤±è´¥: {e}")
                for i, (_, row) in enumerate(best_factors.iterrows(), 1):
                    print(f"{i:2d}. {row['factor_name']}: {row['pearson_ic']:.4f}")
                
        except Exception as e:
            logger.error(f"ç”ŸæˆICæŠ¥å‘Šå¤±è´¥: {str(e)}")


def main():
    """ä¸»å‡½æ•° - ç¨‹åºå…¥å£ç‚¹
    
    æ‰§è¡Œå®Œæ•´çš„ICæ£€æµ‹å·¥ä½œæµï¼š
    1. åˆ›å»ºé…ç½®å¯¹è±¡ï¼ˆåŒ…å«æ‰€æœ‰è¿è¡Œå‚æ•°ï¼‰
    2. åˆå§‹åŒ–æ‰¹é‡å·¥ä½œæµå®ä¾‹
    3. è¿è¡Œå·¥ä½œæµï¼ˆåŒ…å«4ä¸ªä¸»è¦æ­¥éª¤ï¼‰
    4. è¾“å‡ºæ‰§è¡Œç»“æœ
    
    é…ç½®è¯´æ˜ï¼š
    - æµ‹è¯•æ¨¡å¼ï¼š100åªè‚¡ç¥¨ï¼Œ50ä¸ªå› å­ï¼ˆå¿«é€ŸéªŒè¯ï¼‰
    - ç”Ÿäº§æ¨¡å¼ï¼š5000åªè‚¡ç¥¨ï¼Œ158ä¸ªå› å­ï¼ˆå®Œæ•´åˆ†æï¼‰
    """
    # åˆ›å»ºé…ç½®å¯¹è±¡
    config = WorkflowConfig()
    
    # åˆ›å»ºæ‰¹é‡å·¥ä½œæµå®ä¾‹
    workflow = BatchICWorkflow(config)
    
    # è¿è¡Œå·¥ä½œæµ
    success = workflow.run_batch_workflow()
    
    # è¾“å‡ºæ‰§è¡Œç»“æœ
    if success:
        print("ğŸ‰ æ‰¹é‡å·¥ä½œæµæ‰§è¡ŒæˆåŠŸï¼")
        print("ğŸ“ ç»“æœæ–‡ä»¶ä¿å­˜åœ¨: D:\\pythonProject\\æ—¶åºç‰¹å¾å·¥ç¨‹\\source\\ICæ£€æµ‹æ‰¹é‡\\data")
    else:
        print("âŒ æ‰¹é‡å·¥ä½œæµæ‰§è¡Œå¤±è´¥ï¼")
        sys.exit(1)


if __name__ == "__main__":
    main()
